{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e956e7f",
   "metadata": {},
   "source": [
    "# Create Ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6585711d",
   "metadata": {},
   "source": [
    "This code is used to generate the LLM queries for the dense queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "72245fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "29\n",
      "108\n",
      "29\n",
      "113\n",
      "114\n",
      "428\n",
      "114\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "query_files = [\"../data/tot25/subsets/eval20/eval20-queries-dev1.jsonl\", \n",
    "               \"../data/tot25/subsets/eval20/eval20-queries-dev2.jsonl\", \n",
    "               \"../data/tot25/subsets/eval20/eval20-queries-dev3.jsonl\", \n",
    "               \"../data/tot25/subsets/eval20/eval20-queries-train.jsonl\",\n",
    "               \"../data/tot25/subsets/train80/train80-queries-dev1.jsonl\", \n",
    "               \"../data/tot25/subsets/train80/train80-queries-dev2.jsonl\", \n",
    "               \"../data/tot25/subsets/train80/train80-queries-dev3.jsonl\", \n",
    "               \"../data/tot25/subsets/train80/train80-queries-train.jsonl\"\n",
    "               ]\n",
    "\n",
    "counter = 0\n",
    "for file in query_files:\n",
    "    df = pd.read_json(file, lines = True)\n",
    "    print(len(df))\n",
    "    continue\n",
    "    for index, row in df.iterrows():\n",
    "        if counter % 25 == 0:\n",
    "            print(\"\\n\\n\\n\\n\")\n",
    "            print(\"Those are queries for a tip of the tongue request, extract the most important words, which other words are likely to be relevant for a tf-idf algorithm. Give each word a ranking weight for its importance. Remove any unnecessary words. Provide if you have, up to 5 guesses if you know what its refering to. Format the results as jsonl.\")\n",
    "            print('Exampleline of the output: {\"id\":152,\"keywords\":[{\"term\":\"important term in movie\",\"weight\":1.0},{\"term\":\"important term in movie2\",\"weight\":1.0}],\"guesses\":[]}')\n",
    "        counter += 1\n",
    "        print(f\"id: {row['query_id']}: {row['query']}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f370699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1558343\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"term_postings.pkl\", \"rb\") as f:\n",
    "    term_postings = pickle.load(f)\n",
    "\n",
    "print(len(term_postings))\n",
    "\n",
    "OUT_PATH = \"./data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c244431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "from array import array\n",
    "import math \n",
    "OUT_PATH_TF_IDF = \"./data/tf_idf_train.db\"\n",
    "TOTAL_DOCUMENTS = 250_000\n",
    "\n",
    "with shelve.open(OUT_PATH_TF_IDF) as db:\n",
    "    for term, posting in term_postings.items():\n",
    "        # note here counter is the map for document to frequency, not a counter in terms of how many iterations have been executed!\n",
    "        df = len(posting)\n",
    "        \n",
    "        idf = math.log(TOTAL_DOCUMENTS / df)\n",
    "        #  since we sort beforehand the document id and tfs are now sorted the same way as well and belong together with the same index :)\n",
    "        items = sorted(posting.items())\n",
    "        doc_ids = array(\"I\", (doc for doc, _ in items))\n",
    "        # note: for the doc_ids we use I (32 bit integer), as they account for the max range of the document ids and H (16 bit) with a maximum of 65536 (2^16 - 1) is not big enough\n",
    "        # for the term frequencies, since no wikipedia abstract contains nearly as many tokens as that, we use H\n",
    "        # uppercase means unsigned, lower case means signed!\n",
    "        tfs = array('H', (tf for _, tf in items))\n",
    "        db[term] = {\"df\": df, \"idf\": idf, \"doc_ids\": doc_ids, \"tfs\": tfs}\n",
    "    db.sync()\n",
    "    \n",
    "# 20 mins for train, 26:44mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be24297c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1558343\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"index.pkl\", \"rb\") as f:\n",
    "    inverted_index = pickle.load(f)\n",
    "\n",
    "print(len(term_postings))\n",
    "\n",
    "OUT_PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "329cb15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1558343\n",
      "processed 10000 words\n",
      "processed 20000 words\n",
      "processed 30000 words\n",
      "processed 40000 words\n",
      "processed 50000 words\n",
      "processed 60000 words\n",
      "processed 70000 words\n",
      "processed 80000 words\n",
      "processed 90000 words\n",
      "processed 100000 words\n",
      "processed 110000 words\n",
      "processed 120000 words\n",
      "processed 130000 words\n",
      "processed 140000 words\n",
      "processed 150000 words\n",
      "processed 160000 words\n",
      "processed 170000 words\n",
      "processed 180000 words\n",
      "processed 190000 words\n",
      "processed 200000 words\n",
      "processed 210000 words\n",
      "processed 220000 words\n",
      "processed 230000 words\n",
      "processed 240000 words\n",
      "processed 250000 words\n",
      "processed 260000 words\n",
      "processed 270000 words\n",
      "processed 280000 words\n",
      "processed 290000 words\n",
      "processed 300000 words\n",
      "processed 310000 words\n",
      "processed 320000 words\n",
      "processed 330000 words\n",
      "processed 340000 words\n",
      "processed 350000 words\n",
      "processed 360000 words\n",
      "processed 370000 words\n",
      "processed 380000 words\n",
      "processed 390000 words\n",
      "processed 400000 words\n",
      "processed 410000 words\n",
      "processed 420000 words\n",
      "processed 430000 words\n",
      "processed 440000 words\n",
      "processed 450000 words\n",
      "processed 460000 words\n",
      "processed 470000 words\n",
      "processed 480000 words\n",
      "processed 490000 words\n",
      "processed 500000 words\n",
      "processed 510000 words\n",
      "processed 520000 words\n",
      "processed 530000 words\n",
      "processed 540000 words\n",
      "processed 550000 words\n",
      "processed 560000 words\n",
      "processed 570000 words\n",
      "processed 580000 words\n",
      "processed 590000 words\n",
      "processed 600000 words\n",
      "processed 610000 words\n",
      "processed 620000 words\n",
      "processed 630000 words\n",
      "processed 640000 words\n",
      "processed 650000 words\n",
      "processed 660000 words\n",
      "processed 670000 words\n",
      "processed 680000 words\n",
      "processed 690000 words\n",
      "processed 700000 words\n",
      "processed 710000 words\n",
      "processed 720000 words\n",
      "processed 730000 words\n",
      "processed 740000 words\n",
      "processed 750000 words\n",
      "processed 760000 words\n",
      "processed 770000 words\n",
      "processed 780000 words\n",
      "processed 790000 words\n",
      "processed 800000 words\n",
      "processed 810000 words\n",
      "processed 820000 words\n",
      "processed 830000 words\n",
      "processed 840000 words\n",
      "processed 850000 words\n",
      "processed 860000 words\n",
      "processed 870000 words\n",
      "processed 880000 words\n",
      "processed 890000 words\n",
      "processed 900000 words\n",
      "processed 910000 words\n",
      "processed 920000 words\n",
      "processed 930000 words\n",
      "processed 940000 words\n",
      "processed 950000 words\n",
      "processed 960000 words\n",
      "processed 970000 words\n",
      "processed 980000 words\n",
      "processed 990000 words\n",
      "processed 1000000 words\n",
      "processed 1010000 words\n",
      "processed 1020000 words\n",
      "processed 1030000 words\n",
      "processed 1040000 words\n",
      "processed 1050000 words\n",
      "processed 1060000 words\n",
      "processed 1070000 words\n",
      "processed 1080000 words\n",
      "processed 1090000 words\n",
      "processed 1100000 words\n",
      "processed 1110000 words\n",
      "processed 1120000 words\n",
      "processed 1130000 words\n",
      "processed 1140000 words\n",
      "processed 1150000 words\n",
      "processed 1160000 words\n",
      "processed 1170000 words\n",
      "processed 1180000 words\n",
      "processed 1190000 words\n",
      "processed 1200000 words\n",
      "processed 1210000 words\n",
      "processed 1220000 words\n",
      "processed 1230000 words\n",
      "processed 1240000 words\n",
      "processed 1250000 words\n",
      "processed 1260000 words\n",
      "processed 1270000 words\n",
      "processed 1280000 words\n",
      "processed 1290000 words\n",
      "processed 1300000 words\n",
      "processed 1310000 words\n",
      "processed 1320000 words\n",
      "processed 1330000 words\n",
      "processed 1340000 words\n",
      "processed 1350000 words\n",
      "processed 1360000 words\n",
      "processed 1370000 words\n",
      "processed 1380000 words\n",
      "processed 1390000 words\n",
      "processed 1400000 words\n",
      "processed 1410000 words\n",
      "processed 1420000 words\n",
      "processed 1430000 words\n",
      "processed 1440000 words\n",
      "processed 1450000 words\n",
      "processed 1460000 words\n",
      "processed 1470000 words\n",
      "processed 1480000 words\n",
      "processed 1490000 words\n",
      "processed 1500000 words\n",
      "processed 1510000 words\n",
      "processed 1520000 words\n",
      "processed 1530000 words\n",
      "processed 1540000 words\n",
      "processed 1550000 words\n"
     ]
    }
   ],
   "source": [
    "import shelve \n",
    "OUT_PATH_INVERTED_INDEX = \"./data/inverted_index_test.db\"\n",
    "\n",
    "\n",
    "\n",
    "print(len(inverted_index))\n",
    "counter = 0\n",
    "with shelve.open(OUT_PATH_INVERTED_INDEX) as db:\n",
    "    for term, posting_list in inverted_index.items():\n",
    "        db[term] = posting_list\n",
    "        counter += 1\n",
    "        if counter % 10_000 == 0:\n",
    "            print(f\"processed {counter} words\")\n",
    "    db.sync()\n",
    "\n",
    "#28 mins for train, 24 for eval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
