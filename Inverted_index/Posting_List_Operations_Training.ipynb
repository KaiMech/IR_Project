{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fceeb238",
   "metadata": {},
   "source": [
    "# Posting List operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440fc3f",
   "metadata": {},
   "source": [
    "This notebook is to evaluate inverted index and tf_idf on any given datasets.\n",
    "For usage adapt the last 4 variables in the first cell (inverted_index_path, tf_idf_path, out_path and query_files) accordingly, depending on whether everything should be executed fo train or eval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f751290",
   "metadata": {},
   "source": [
    "todos:\n",
    "-   review everything\n",
    "-   run for train and test set\n",
    "-   check output file parameter if rank -> tf_idf is correct or this is meant elsewise --> Ciwan confirmed is fine, must just be sorted :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5176647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import shelve\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math \n",
    "from collections import Counter\n",
    "from collections import defaultdict \n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    import orjson as _json\n",
    "    loads = _json.loads\n",
    "except Exception:\n",
    "    import json as _json\n",
    "    loads = _json.loads\n",
    "\n",
    "INVERTED_INDEX_PATH = \"./data/inverted_index_eval.db\"\n",
    "TF_IDF_PATH = \"C:/Users/Kai/Desktop/safetyCopyInformationRetrievalDBs/tf_idf_eval.db\"  # !!!!!!!!! ADAPT TO NEW TF_IDF_EVAL.DB\n",
    "OUT_PATH = \"../data/runs\"\n",
    "query_files = [\"../data/tot25/subsets/eval20/eval20-queries-dev1.jsonl\", \"../data/tot25/subsets/eval20/eval20-queries-dev2.jsonl\", \"../data/tot25/subsets/eval20/eval20-queries-dev3.jsonl\", \"../data/tot25/subsets/eval20/eval20-queries-train.jsonl\"]\n",
    "stop = set(stopwords.words('english') + list(string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e6477",
   "metadata": {},
   "source": [
    "## Function for Lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "499def07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'notebook', 'be', 'use', 'to', 'create', 'an', 'inverted_index', 'and', 'tf_idf', 'database', 'for', 'the', 'provided', 'corpus', 'file', '(', 'train', 'or', 'eval', ')', '.', 'adapt', 'the', 'variable', 'in', 'the', 'first', 'cell', 'accordingly', 'for', 'file', 'and', 'output', 'dir.note', 'that', 'the', 'executino', 'time', 'for', 'eval', 'be', '1/3', 'that', 'of', 'train', ',', 'despite', 'the', 'fact', 'that', 'eval', 'contain', '2.5', 'time', 'the', 'amount', 'of', 'doc', '.', 'we', 'assume', 'this', 'be']\n"
     ]
    }
   ],
   "source": [
    "# download for lematization\n",
    "nltk.download('punkt_tab')   \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def get_wordnet_pos(tag): # mapping POS tag from pos_tag to a format WordNetLemmatizer accepts.\n",
    "    match tag[0]:\n",
    "        case 'J':\n",
    "            return wordnet.ADJ\n",
    "        case 'V':\n",
    "            return wordnet.VERB\n",
    "        #case 'N':\n",
    "        #    return wordnet.NOUN       online source: here, but twice no benefit so removed\n",
    "        case 'R':\n",
    "            return wordnet.ADV\n",
    "        case _:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lematization(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    return [lemmatizer.lemmatize(token, get_wordnet_pos(tag)).lower() for token, tag in tagged] # lower so can compare eg write friday instead of Friday\n",
    "\n",
    "#sanity check\n",
    "lem = lematization(\"This notebook is used to create an inverted_index and tf_idf database for the provided corpus file (train or eval). Adapt the variables in the first cell accordingly for files and output dir.Note that the executino time for eval is 1/3 that of train, despite the fact that eval contains 2.5 times the amount of docs. We assume this is \")\n",
    "print(lem) # later need to remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b7850e",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14994e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to tokenize and preprocess a document\n",
    "def preprocess_unique(text):\n",
    "    #old function changed to lematization    tokens = set(word_tokenize(text.lower())) # get all tokens\n",
    "    tokens = set(lematization(text))\n",
    "    return [i for i in tokens if i not in stop] # get all tokens without stopwords\n",
    "\n",
    "def AND(l1, l2):\n",
    "    # note: this method assumes that both lists are sorted and do not contain any duplicates (which if the input comes from a posting list is given)\n",
    "    # this asssumption is given considering the inverted index and tf_idf db's were created\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    sqrtJump1 = max(math.isqrt(len(l1)), 1)\n",
    "    sqrtJump2 = max(math.isqrt(len(l2)), 1)\n",
    "    result = []\n",
    "    while p1 < len(l1)  and p2 < len(l2):\n",
    "        if l1[p1] == l2[p2]:\n",
    "            result.append(l1[p1])\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "        # skip pointer in first list\n",
    "        elif p1 % sqrtJump1 == 0 and p1 + sqrtJump1 < len(l1) and l1[p1 + sqrtJump1] <= l2[p2]:\n",
    "            p1 += sqrtJump1\n",
    "        # skip pointer in second list\n",
    "        elif p2 % sqrtJump2 == 0 and p2 + sqrtJump2 < len(l2) and l2[p2 + sqrtJump2] <= l1[p1]:\n",
    "            p2 += sqrtJump2\n",
    "        elif l1[p1] < l2[p2]:\n",
    "            p1 += 1\n",
    "        else:\n",
    "            p2 += 1\n",
    "    return result\n",
    "\n",
    "def OR(l1, l2):\n",
    "    # note: this method assumes that both lists are sorted and do not contain any duplicates (which if the input comes from a posting list is given)\n",
    "    # this asssumption is given considering the inverted index and tf_idf db's were created\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    result = []\n",
    "    while p1 < len(l1) and p2 < len(l2):\n",
    "        if l1[p1] < l2[p2]:\n",
    "            result.append(l1[p1])\n",
    "            p1 += 1\n",
    "        elif l1[p1] > l2[p2]:\n",
    "            result.append(l2[p2])\n",
    "            p2 += 1\n",
    "        else: \n",
    "            result.append(l1[p1])\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "    while p1 < len(l1):\n",
    "        result.append(l1[p1])\n",
    "        p1 += 1\n",
    "    while p2 < len(l2):\n",
    "        result.append(l2[p2])\n",
    "        p2 += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af8353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/tot25/subsets/eval20/eval20-queries-dev1.jsonl\n",
      "2025-11-17 22:44:36.277845\n",
      "../data/tot25/subsets/eval20/eval20-queries-dev2.jsonl\n",
      "2025-11-17 22:44:50.405052\n",
      "../data/tot25/subsets/eval20/eval20-queries-dev3.jsonl\n",
      "2025-11-17 22:45:00.711880\n",
      "../data/tot25/subsets/eval20/eval20-queries-train.jsonl\n",
      "2025-11-17 22:46:12.455231\n"
     ]
    }
   ],
   "source": [
    "def query_inverted_index_and(query):\n",
    "    with shelve.open(INVERTED_INDEX_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique(query)]\n",
    "        terms.sort(key = lambda t:len(db[t]))\n",
    "        result = db[terms[0]]\n",
    "        for term in terms[1:]:\n",
    "            result = AND(result, db[term])\n",
    "        return result\n",
    "\n",
    "def query_inverted_index_sorted_by_word_counts_top_n(query, n):\n",
    "    with shelve.open(INVERTED_INDEX_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique(query)]\n",
    "        result = list()\n",
    "        for term in terms:\n",
    "            if not term in db:\n",
    "                continue\n",
    "            result.extend(db[term])\n",
    "        return Counter(result).most_common(n)\n",
    "    \n",
    "def evaluate_inverted_index(query_files, runID, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True) \n",
    "    for filename in query_files:\n",
    "        print(filename)\n",
    "        print(datetime.now())\n",
    "        with open(output_path + \"/\" + filename.split(\"/\")[-1].split(\".\")[0] + \".run\", \"w\") as output_file:\n",
    "            testdata = pd.read_json(filename, lines = True)\n",
    "            for index, row in testdata.iterrows():\n",
    "                result = query_inverted_index_sorted_by_word_counts_top_n(row[\"query\"], 1000)\n",
    "                rank_counter = 1\n",
    "                for id, frequency in result:\n",
    "                    output_file.write(f\"{row.get('query_id')} Q0 {id} {rank_counter} {frequency} {runID}\\n\")\n",
    "                    # print(f\"{row.get(\"query_id\")} Q0 {id} {rank_counter} {frequency} {runID}\")\n",
    "                    rank_counter += 1\n",
    "                \n",
    "                \n",
    "\n",
    "evaluate_inverted_index(query_files, 1, OUT_PATH + \"/inverted_index\")\n",
    "# 2 mins :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be1f53",
   "metadata": {},
   "source": [
    "## TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb95c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_unique_list(text):\n",
    "    #old function changed to lematization    tokens = list(word_tokenize(text.lower())) # get all tokens\n",
    "    tokens = list(lematization(text)) # get all tokens\n",
    "    return [i for i in tokens if i not in stop] # get all tokens without stopwords\n",
    "\n",
    "\n",
    "def query_tf_idf_top_n(query, n):\n",
    "    with shelve.open(TF_IDF_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique_list(query)]\n",
    "        \n",
    "        tf_idf_sums = defaultdict(float)\n",
    "        \n",
    "        for term in terms:\n",
    "            if term not in db:\n",
    "                continue\n",
    "            data = db[term]\n",
    "            idf = data[\"idf\"]\n",
    "            doc_ids = data[\"doc_ids\"]\n",
    "            tfs = data[\"tfs\"]\n",
    "\n",
    "            # in the database the tfs for each document is stored for the documents index in doc_ids, for this reason we can easily zip them together :)\n",
    "            for doc_id, tf in zip(doc_ids, tfs):\n",
    "                tf_idf_sums[doc_id] +=  tf * idf\n",
    "        \n",
    "        return sorted(tf_idf_sums.items(), key = lambda x: x[1], reverse = True)[:n]\n",
    "\n",
    "def evaluate_tf_idf(query_files, runID, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for filename in query_files:\n",
    "        print(filename)\n",
    "        print(datetime.now())\n",
    "        with open(output_path + \"/\" + filename.split(\"/\")[-1].split(\".\")[0] + \".run\", \"w\") as output_file:\n",
    "            testdata = pd.read_json(filename, lines = True)\n",
    "            for index, row in testdata.iterrows():\n",
    "                result = query_tf_idf_top_n(row[\"query\"], 1000)\n",
    "                rank_counter = 1\n",
    "                for id, tf_idf in result:\n",
    "                    # output_file.write(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\\n\")\n",
    "                    print(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\")\n",
    "                    rank_counter += 1\n",
    "            \n",
    "evaluate_tf_idf(query_files, 1, OUT_PATH + \"/tf_idf\")\n",
    "# 3.5 mins :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f539eb",
   "metadata": {},
   "source": [
    "# Reduced queries without weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4057616",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = \"../data/runs/Reduced_Query\"\n",
    "\n",
    "def preprocess_unique_list(text):\n",
    "    #old function changed to lematization    tokens = list(word_tokenize(text.lower())) # get all tokens\n",
    "    tokens = list(lematization(text)) # get all tokens\n",
    "    return [i for i in tokens if i not in stop] # get all tokens without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f552725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/tot25/subsets/eval20/eval20-queries-dev1.jsonl\n",
      "2025-11-24 12:11:15.890681\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lematization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m                     \u001b[38;5;66;03m# print(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\")\u001b[39;00m\n\u001b[32m     39\u001b[39m                     rank_counter += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mevaluate_tf_idf_reduced_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUT_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/tf_idf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mevaluate_tf_idf_reduced_query\u001b[39m\u001b[34m(query_files, runID, output_path)\u001b[39m\n\u001b[32m     32\u001b[39m testdata = pd.read_json(filename, lines = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m testdata.iterrows():\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     result = \u001b[43mquery_tf_idf_top_n\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_improved_queries\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_improved_queries\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mterms_string\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     rank_counter = \u001b[32m1\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, tf_idf \u001b[38;5;129;01min\u001b[39;00m result:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mquery_tf_idf_top_n\u001b[39m\u001b[34m(query, n)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery_tf_idf_top_n\u001b[39m(query, n):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m shelve.open(TF_IDF_PATH) \u001b[38;5;28;01mas\u001b[39;00m db:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         terms = [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpreprocess_unique_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m      5\u001b[39m         tf_idf_sums = defaultdict(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m terms:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mpreprocess_unique_list\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_unique_list\u001b[39m(text):\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m#old function changed to lematization    tokens = list(word_tokenize(text.lower())) # get all tokens\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     tokens = \u001b[38;5;28mlist\u001b[39m(\u001b[43mlematization\u001b[49m(text)) \u001b[38;5;66;03m# get all tokens\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop]\n",
      "\u001b[31mNameError\u001b[39m: name 'lematization' is not defined"
     ]
    }
   ],
   "source": [
    "def query_tf_idf_top_n(query, n):\n",
    "    with shelve.open(TF_IDF_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique_list(query)]\n",
    "        \n",
    "        tf_idf_sums = defaultdict(float)\n",
    "        \n",
    "        for term in terms:\n",
    "            if term not in db:\n",
    "                continue\n",
    "            data = db[term]\n",
    "            idf = data[\"idf\"]\n",
    "            doc_ids = data[\"doc_ids\"]\n",
    "            tfs = data[\"tfs\"]\n",
    "\n",
    "            # in the database the tfs for each document is stored for the documents index in doc_ids, for this reason we can easily zip them together :)\n",
    "            for doc_id, tf in zip(doc_ids, tfs):\n",
    "                tf_idf_sums[doc_id] +=  tf * idf\n",
    "        \n",
    "        return sorted(tf_idf_sums.items(), key = lambda x: x[1], reverse = True)[:n]\n",
    "\n",
    "def evaluate_tf_idf_reduced_query(query_files, runID, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for filename in query_files:\n",
    "        print(filename)\n",
    "        print(datetime.now())\n",
    "        filename_improved_query = \"./data/improved_queries/\" + re.search(r'([^/]+)(?=\\.[^.]+$)', filename).group(1) + \"_improved_queries.jsonl\"\n",
    "        df_improved_queries = pd.read_json(filename_improved_query, lines = True)\n",
    "        df_improved_queries[\"terms_string\"] = df_improved_queries[\"keywords\"].apply(lambda items: \" \".join([d[\"term\"] for d in items]))\n",
    "        \n",
    "        \n",
    "        with open(output_path + \"/\" + filename.split(\"/\")[-1].split(\".\")[0] + \".run\", \"w\") as output_file:\n",
    "            testdata = pd.read_json(filename, lines = True)\n",
    "            for index, row in testdata.iterrows():\n",
    "                result = query_tf_idf_top_n(df_improved_queries[df_improved_queries[\"id\"] == row.get(\"query_id\")][\"terms_string\"].iloc[0], 1000)\n",
    "                rank_counter = 1\n",
    "                for id, tf_idf in result:\n",
    "                    output_file.write(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\\n\")\n",
    "                    # print(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\")\n",
    "                    rank_counter += 1\n",
    "            \n",
    "evaluate_tf_idf_reduced_query(query_files, 1, OUT_PATH + \"/tf_idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d376b",
   "metadata": {},
   "source": [
    "# Distance Weighted retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "lst = [10, 10, 10, 10,10, 10, 10, 10,10, 10, 10, 10,10, 10, 10, 10]\n",
    "k = 0.1*(1/len(lst))\n",
    "weights = [math.exp(-k * i) for i in range(len(lst))]\n",
    "weighted_sum = sum(x * w for x, w in zip(lst, weights))\n",
    "print(weights)\n",
    "print(weighted_sum)\n",
    "l= 35\n",
    "print(math.exp(-1*(1/l)*l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ceb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_WEIGHT = 0.4 \n",
    "def query_tf_idf_top_n(query, n):\n",
    "    with shelve.open(TF_IDF_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique_list(query)]\n",
    "        term_count = len(terms)\n",
    "        \n",
    "        tf_idf_sums = defaultdict(float)\n",
    "        \n",
    "        counter = 0\n",
    "        \n",
    "        for term in terms:\n",
    "            if term not in db:\n",
    "                continue\n",
    "            data = db[term]\n",
    "            idf = data[\"idf\"]\n",
    "            doc_ids = data[\"doc_ids\"]\n",
    "            tfs = data[\"tfs\"]\n",
    "\n",
    "            # in the database the tfs for each document is stored for the documents index in doc_ids, for this reason we can easily zip them together :)\n",
    "            for doc_id, tf in zip(doc_ids, tfs):\n",
    "                tf_idf_sums[doc_id] +=  tf * idf * (1 - (1 - MIN_WEIGHT) * ((counter / term_count) ** 2))\n",
    "            counter += 1\n",
    "        \n",
    "        return sorted(tf_idf_sums.items(), key = lambda x: x[1], reverse = True)[:n]\n",
    "\n",
    "def evaluate_tf_idf_reduced_query(query_files, runID, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for filename in query_files:\n",
    "        print(filename)\n",
    "        print(datetime.now())\n",
    "        filename_improved_query = \"./data/improved_queries/\" + re.search(r'([^/]+)(?=\\.[^.]+$)', filename).group(1) + \"_improved_queries.jsonl\"\n",
    "        df_improved_queries = pd.read_json(filename_improved_query, lines = True)\n",
    "        df_improved_queries[\"terms_string\"] = df_improved_queries[\"keywords\"].apply(lambda items: \" \".join([d[\"term\"] for d in items]))\n",
    "        \n",
    "        \n",
    "        with open(output_path + \"/\" + filename.split(\"/\")[-1].split(\".\")[0] + \".run\", \"w\") as output_file:\n",
    "            testdata = pd.read_json(filename, lines = True)\n",
    "            for index, row in testdata.iterrows():\n",
    "                result = query_tf_idf_top_n(df_improved_queries[df_improved_queries[\"id\"] == row.get(\"query_id\")][\"terms_string\"].iloc[0], 1000)\n",
    "                rank_counter = 1\n",
    "                for id, tf_idf in result:\n",
    "                    output_file.write(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\\n\")\n",
    "                    # print(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\")\n",
    "                    rank_counter += 1\n",
    "            \n",
    "evaluate_tf_idf_reduced_query(query_files, 1, OUT_PATH + \"/tf_idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c75c5",
   "metadata": {},
   "source": [
    "# Reduced queries weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = \"../data/runs/Reduced_Query_with_weights\"\n",
    "# todo: adapt to log based version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed8b3c",
   "metadata": {},
   "source": [
    "No we also account for the weights given by ChatGPT. For each proposed phrase, like \"action thriller\", we preprocess it and split it into unique terms, then compute the min tf over all terms and the maximum corresponding idf over all unique terms in the phrase, based on that we compute a weighted sum tf-idf over all proposed phrases.\n",
    "1. minimum tf: since we do not store phrases in our tf-idf, but only unique tokens, we make the assumption that the given terms in a phrase co-occur in the amount of the minimum tf  of the terms. We are aware that this might not be the case, however due to computational constraints and for storage reasons we cannot compute the tf-idf based on phrases, which is why we make this assumption.\n",
    "2. maximum idf: Since in (1) we assume that the occurence of the phrase is defined by the least occuring term in the phrase, we take the maximum idf, as this corresponds to the least occuring term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd0e831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/tot25/subsets/eval20/eval20-queries-dev1.jsonl\n",
      "2025-11-24 14:43:08.558426\n",
      "152 Q0 294286 1 2041.425132765962 1\n",
      "153 Q0 294286 1 2762.1759519940006 1\n",
      "185 Q0 5881047 1 1487.0900562199263 1\n",
      "240 Q0 101888 1 1191.322835987472 1\n",
      "365 Q0 9006392 1 1350.0957854021683 1\n",
      "385 Q0 1257797 1 1401.5966458153616 1\n",
      "391 Q0 6796416 1 751.3415512305727 1\n",
      "403 Q0 294286 1 2626.971930183811 1\n",
      "442 Q0 294286 1 1846.2288641878026 1\n",
      "458 Q0 4059850 1 1597.1308770770863 1\n",
      "490 Q0 3333003 1 1284.1812690442857 1\n",
      "498 Q0 8557881 1 997.9155049356289 1\n",
      "512 Q0 39326926 1 1341.2780741732608 1\n",
      "554 Q0 74998017 1 1792.303147766898 1\n",
      "563 Q0 26349927 1 1546.5420447407168 1\n",
      "625 Q0 19872006 1 2107.4533284686695 1\n",
      "639 Q0 142340 1 1108.4030682821847 1\n",
      "722 Q0 294286 1 2636.6703867704086 1\n",
      "735 Q0 246042 1 1308.0301624892556 1\n",
      "761 Q0 1390880 1 838.1127741735284 1\n",
      "766 Q0 1303939 1 1243.382699942382 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m                     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     43\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mevaluate_tf_idf_reduced_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUT_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/tf_idf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mevaluate_tf_idf_reduced_query\u001b[39m\u001b[34m(query_files, runID, output_path)\u001b[39m\n\u001b[32m     34\u001b[39m testdata = pd.read_json(filename, lines = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m testdata.iterrows():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     result = \u001b[43mquery_tf_idf_top_n\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_improved_queries\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_improved_queries\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkeywords\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     rank_counter = \u001b[32m1\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, tf_idf \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[32m     39\u001b[39m         \u001b[38;5;66;03m# output_file.write(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\\n\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mquery_tf_idf_top_n\u001b[39m\u001b[34m(terms_weights, n)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m datas:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m doc_id, tf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data[\u001b[33m\"\u001b[39m\u001b[33mdoc_ids\u001b[39m\u001b[33m\"\u001b[39m], data[\u001b[33m\"\u001b[39m\u001b[33mtfs\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         min_tfs[doc_id] = \u001b[38;5;28mmin\u001b[39m(\u001b[43mmin_tfs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[43m]\u001b[49m, tf)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc_id \u001b[38;5;129;01min\u001b[39;00m doc_ids:\n\u001b[32m     20\u001b[39m     tf_idf_sums[doc_id] += min_tfs[doc_id] *  idf * entry[\u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mquery_tf_idf_top_n.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     13\u001b[39m idf = \u001b[38;5;28mmin\u001b[39m(data[\u001b[33m\"\u001b[39m\u001b[33midf\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m datas)\n\u001b[32m     14\u001b[39m doc_ids = \u001b[38;5;28mset\u001b[39m().union(*(data[\u001b[33m\"\u001b[39m\u001b[33mdoc_ids\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m datas))\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m min_tfs = defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m datas:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m doc_id, tf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data[\u001b[33m\"\u001b[39m\u001b[33mdoc_ids\u001b[39m\u001b[33m\"\u001b[39m], data[\u001b[33m\"\u001b[39m\u001b[33mtfs\u001b[39m\u001b[33m\"\u001b[39m]):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def query_tf_idf_top_n(terms_weights, n):\n",
    "    with shelve.open(TF_IDF_PATH) as db:\n",
    "        tf_idf_sums = defaultdict(float) # default value is 0 :)\n",
    "        for entry in terms_weights:\n",
    "            datas = []\n",
    "            # db does not contain subterm --> ignore it for quering\n",
    "            preprocessed_split = preprocess_unique_list(entry[\"term\"])\n",
    "            if any(term not in db for term in preprocessed_split):\n",
    "                continue\n",
    "            for term in preprocessed_split:\n",
    "                datas.append(db[term])\n",
    "            idf = max(data[\"idf\"] for data in datas)\n",
    "            doc_ids = set().union(*(data[\"doc_ids\"] for data in datas))\n",
    "            min_tfs = defaultdict(lambda: float(\"inf\"))\n",
    "            for data in datas:\n",
    "                for doc_id, tf in zip(data[\"doc_ids\"], data[\"tfs\"]):\n",
    "                    min_tfs[doc_id] = min(min_tfs[doc_id], tf)\n",
    "            for doc_id in doc_ids:\n",
    "                tf_idf_sums[doc_id] += min_tfs[doc_id] *  idf * entry[\"weight\"]\n",
    "                \n",
    "        return sorted(tf_idf_sums.items(), key = lambda x: x[1], reverse = True)[:n]\n",
    "   \n",
    "\n",
    "def evaluate_tf_idf_reduced_query(query_files, runID, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for filename in query_files:\n",
    "        print(filename)\n",
    "        print(datetime.now())\n",
    "        filename_improved_query = \"./data/improved_queries/\" + re.search(r'([^/]+)(?=\\.[^.]+$)', filename).group(1) + \"_improved_queries.jsonl\"\n",
    "        df_improved_queries = pd.read_json(filename_improved_query, lines = True)\n",
    "        \n",
    "        with open(output_path + \"/\" + filename.split(\"/\")[-1].split(\".\")[0] + \".run\", \"w\") as output_file:\n",
    "            testdata = pd.read_json(filename, lines = True)\n",
    "            for index, row in testdata.iterrows():\n",
    "                result = query_tf_idf_top_n(df_improved_queries[df_improved_queries[\"id\"] == row.get(\"query_id\")].iloc[0][\"keywords\"], 1000)\n",
    "                rank_counter = 1\n",
    "                for id, tf_idf in result:\n",
    "                    # output_file.write(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\\n\")\n",
    "                    print(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\")\n",
    "                    rank_counter += 1\n",
    "                    break # remove at the end!!!\n",
    "            break # remove at the end \n",
    "            \n",
    "evaluate_tf_idf_reduced_query(query_files, 1, OUT_PATH + \"/tf_idf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
