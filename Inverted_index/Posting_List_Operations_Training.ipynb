{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fceeb238",
   "metadata": {},
   "source": [
    "# Posting List operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440fc3f",
   "metadata": {},
   "source": [
    "This notebook is to evaluate inverted index and tf_idf on any given datasets.\n",
    "For usage adapt the last 4 variables in the first cell (inverted_index_path, tf_idf_path, out_path and query_files) accordingly, depending on whether everything should be executed fo train or eval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f751290",
   "metadata": {},
   "source": [
    "todos:\n",
    "-   review everything\n",
    "-   run for train and test set\n",
    "-   check output file parameter if rank -> tf_idf is correct or this is meant elsewise --> Ciwan confirmed is fine, must just be sorted :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5176647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import shelve\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math \n",
    "from collections import Counter\n",
    "from collections import defaultdict \n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    import orjson as _json\n",
    "    loads = _json.loads\n",
    "except Exception:\n",
    "    import json as _json\n",
    "    loads = _json.loads\n",
    "\n",
    "INVERTED_INDEX_PATH = \"./data/inverted_index_train.db\"\n",
    "TF_IDF_PATH = \"./data/tf_idf_train.db\"\n",
    "OUT_PATH = \"../data/runs\"\n",
    "query_files = [\"../data/tot25/subsets/train80/train80-queries-dev1.jsonl\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b7850e",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14994e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "# Function to tokenize and preprocess a document\n",
    "def preprocess_unique(text):\n",
    "    tokens = set(word_tokenize(text.lower())) # get all tokens\n",
    "    return [i for i in tokens if i not in stop] # get all tokens without stopwords\n",
    "\n",
    "def AND(l1, l2):\n",
    "    # note: this method assumes that both lists are sorted and do not contain any duplicates (which if the input comes from a posting list is given)\n",
    "    # this asssumption is given considering the inverted index and tf_idf db's were created\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    sqrtJump1 = max(math.isqrt(len(l1)), 1)\n",
    "    sqrtJump2 = max(math.isqrt(len(l2)), 1)\n",
    "    result = []\n",
    "    while p1 < len(l1)  and p2 < len(l2):\n",
    "        if l1[p1] == l2[p2]:\n",
    "            result.append(l1[p1])\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "        # skip pointer in first list\n",
    "        elif p1 % sqrtJump1 == 0 and p1 + sqrtJump1 < len(l1) and l1[p1 + sqrtJump1] <= l2[p2]:\n",
    "            p1 += sqrtJump1\n",
    "        # skip pointer in second list\n",
    "        elif p2 % sqrtJump2 == 0 and p2 + sqrtJump2 < len(l2) and l2[p2 + sqrtJump2] <= l1[p1]:\n",
    "            p2 += sqrtJump2\n",
    "        elif l1[p1] < l2[p2]:\n",
    "            p1 += 1\n",
    "        else:\n",
    "            p2 += 1\n",
    "    return result\n",
    "\n",
    "def OR(l1, l2):\n",
    "    # note: this method assumes that both lists are sorted and do not contain any duplicates (which if the input comes from a posting list is given)\n",
    "    # this asssumption is given considering the inverted index and tf_idf db's were created\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    result = []\n",
    "    while p1 < len(l1) and p2 < len(l2):\n",
    "        if l1[p1] < l2[p2]:\n",
    "            result.append(l1[p1])\n",
    "            p1 += 1\n",
    "        elif l1[p1] > l2[p2]:\n",
    "            result.append(l2[p2])\n",
    "            p2 += 1\n",
    "        else: \n",
    "            result.append(l1[p1])\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "    while p1 < len(l1):\n",
    "        result.append(l1[p1])\n",
    "        p1 += 1\n",
    "    while p2 < len(l2):\n",
    "        result.append(l2[p2])\n",
    "        p2 += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_inverted_index_and(query):\n",
    "    with shelve.open(INVERTED_INDEX_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique(query)]\n",
    "        terms.sort(key = lambda t:len(db[t]))\n",
    "        result = db[terms[0]]\n",
    "        for term in terms[1:]:\n",
    "            result = AND(result, db[term])\n",
    "        return result\n",
    "\n",
    "def query_inverted_index_sorted_by_word_counts_top_n(query, n):\n",
    "    with shelve.open(INVERTED_INDEX_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique(query)]\n",
    "        result = list()\n",
    "        for term in terms:\n",
    "            if not term in db:\n",
    "                continue\n",
    "            result.extend(db[term])\n",
    "        return Counter(result).most_common(n)\n",
    "    \n",
    "def evaluate_inverted_index(query_files, runID, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True) \n",
    "    for filename in query_files:\n",
    "        with open(output_path + \"/\" + filename.split(\"/\")[-1].split(\".\")[0] + \".txt\", \"w\") as output_file:\n",
    "            testdata = pd.read_json(filename, lines = True)\n",
    "            for index, row in testdata.iterrows():\n",
    "                result = query_inverted_index_sorted_by_word_counts_top_n(row[\"query\"], 1000)\n",
    "                rank_counter = 1\n",
    "                for id, frequency in result:\n",
    "                    output_file.write(f\"{row.get('query_id')} Q0 {id} {rank_counter} {frequency} {runID}\\n\")\n",
    "                    # print(f\"{row.get(\"query_id\")} Q0 {id} {rank_counter} {frequency} {runID}\")\n",
    "                    rank_counter += 1\n",
    "                \n",
    "                \n",
    "\n",
    "# evaluate_inverted_index(query_files, 1, OUT_PATH + \"/inverted_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be1f53",
   "metadata": {},
   "source": [
    "## TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb95c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_PATH = \"./data/tf_idf.db\"\n",
    "            \n",
    "def query_tf_idf_top_n(query, n):\n",
    "    with shelve.open(TF_IDF_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique(query)]\n",
    "        \n",
    "        tf_idf_sums = defaultdict(float)\n",
    "        \n",
    "        for term in terms:\n",
    "            if term not in db:\n",
    "                continue\n",
    "            data = db[term]\n",
    "            idf = data[\"idf\"]\n",
    "            doc_ids = data[\"doc_ids\"]\n",
    "            tfs = data[\"tfs\"]\n",
    "\n",
    "            # in the database the tfs for each document is stored for the documents index in doc_ids, for this reason we can easily zip them together :)\n",
    "            for doc_id, tf in zip(doc_ids, tfs):\n",
    "                tf_idf_sums[doc_id] +=  tf * idf\n",
    "        \n",
    "        return sorted(tf_idf_sums.items(), key = lambda x: x[1], reverse = True)[:n]\n",
    "\n",
    "def evaluate_tf_idf(query_files, runID, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for filename in query_files:\n",
    "        with open(output_path + \"/\" + filename.split(\"/\")[-1].split(\".\")[0] + \".txt\", \"w\") as output_file:\n",
    "            testdata = pd.read_json(filename, lines = True)\n",
    "            for index, row in testdata.iterrows():\n",
    "                result = query_tf_idf_top_n(row[\"query\"], 1000)\n",
    "                rank_counter = 1\n",
    "                for id, tf_idf in result:\n",
    "                        output_file.write(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\\n\")\n",
    "                        # print(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\")\n",
    "                        rank_counter += 1\n",
    "            \n",
    "            \n",
    "\n",
    "# evaluate_tf_idf(query_files, 1, OUT_PATH + \"/tf_idf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
