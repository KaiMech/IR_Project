{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fceeb238",
   "metadata": {},
   "source": [
    "# Posting List operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440fc3f",
   "metadata": {},
   "source": [
    "This notebook is to evaluate inverted index and tf_idf on any given datasets.\n",
    "For usage adapt the last 4 variables in the first cell (inverted_index_path, tf_idf_path, out_path and query_files) accordingly, depending on whether everything should be executed fo train or eval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f751290",
   "metadata": {},
   "source": [
    "todos:\n",
    "-   review everything\n",
    "-   run for train and test set\n",
    "-   check output file parameter if rank -> tf_idf is correct or this is meant elsewise --> Ciwan confirmed is fine, must just be sorted :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5176647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import shelve\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math \n",
    "from collections import Counter\n",
    "from collections import defaultdict \n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    import orjson as _json\n",
    "    loads = _json.loads\n",
    "except Exception:\n",
    "    import json as _json\n",
    "    loads = _json.loads\n",
    "\n",
    "INVERTED_INDEX_PATH = \"./data/inverted_index_eval.db\"\n",
    "TF_IDF_PATH = \"./data/tf_idf_eval.db\"\n",
    "OUT_PATH = \"../data/runs\"\n",
    "query_files = [\"../data/tot25/subsets/eval20/eval20-queries-dev1.jsonl\", \"../data/tot25/subsets/eval20/eval20-queries-dev2.jsonl\", \"../data/tot25/subsets/eval20/eval20-queries-dev3.jsonl\", \"../data/tot25/subsets/eval20/eval20-queries-train.jsonl\"]\n",
    "stop = set(stopwords.words('english') + list(string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b7850e",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14994e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to tokenize and preprocess a document\n",
    "def preprocess_unique(text):\n",
    "    tokens = set(word_tokenize(text.lower())) # get all tokens\n",
    "    return [i for i in tokens if i not in stop] # get all tokens without stopwords\n",
    "\n",
    "def AND(l1, l2):\n",
    "    # note: this method assumes that both lists are sorted and do not contain any duplicates (which if the input comes from a posting list is given)\n",
    "    # this asssumption is given considering the inverted index and tf_idf db's were created\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    sqrtJump1 = max(math.isqrt(len(l1)), 1)\n",
    "    sqrtJump2 = max(math.isqrt(len(l2)), 1)\n",
    "    result = []\n",
    "    while p1 < len(l1)  and p2 < len(l2):\n",
    "        if l1[p1] == l2[p2]:\n",
    "            result.append(l1[p1])\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "        # skip pointer in first list\n",
    "        elif p1 % sqrtJump1 == 0 and p1 + sqrtJump1 < len(l1) and l1[p1 + sqrtJump1] <= l2[p2]:\n",
    "            p1 += sqrtJump1\n",
    "        # skip pointer in second list\n",
    "        elif p2 % sqrtJump2 == 0 and p2 + sqrtJump2 < len(l2) and l2[p2 + sqrtJump2] <= l1[p1]:\n",
    "            p2 += sqrtJump2\n",
    "        elif l1[p1] < l2[p2]:\n",
    "            p1 += 1\n",
    "        else:\n",
    "            p2 += 1\n",
    "    return result\n",
    "\n",
    "def OR(l1, l2):\n",
    "    # note: this method assumes that both lists are sorted and do not contain any duplicates (which if the input comes from a posting list is given)\n",
    "    # this asssumption is given considering the inverted index and tf_idf db's were created\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    result = []\n",
    "    while p1 < len(l1) and p2 < len(l2):\n",
    "        if l1[p1] < l2[p2]:\n",
    "            result.append(l1[p1])\n",
    "            p1 += 1\n",
    "        elif l1[p1] > l2[p2]:\n",
    "            result.append(l2[p2])\n",
    "            p2 += 1\n",
    "        else: \n",
    "            result.append(l1[p1])\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "    while p1 < len(l1):\n",
    "        result.append(l1[p1])\n",
    "        p1 += 1\n",
    "    while p2 < len(l2):\n",
    "        result.append(l2[p2])\n",
    "        p2 += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af8353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/tot25/subsets/eval20/eval20-queries-dev1.jsonl\n",
      "2025-11-17 22:44:36.277845\n",
      "../data/tot25/subsets/eval20/eval20-queries-dev2.jsonl\n",
      "2025-11-17 22:44:50.405052\n",
      "../data/tot25/subsets/eval20/eval20-queries-dev3.jsonl\n",
      "2025-11-17 22:45:00.711880\n",
      "../data/tot25/subsets/eval20/eval20-queries-train.jsonl\n",
      "2025-11-17 22:46:12.455231\n"
     ]
    }
   ],
   "source": [
    "def query_inverted_index_and(query):\n",
    "    with shelve.open(INVERTED_INDEX_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique(query)]\n",
    "        terms.sort(key = lambda t:len(db[t]))\n",
    "        result = db[terms[0]]\n",
    "        for term in terms[1:]:\n",
    "            result = AND(result, db[term])\n",
    "        return result\n",
    "\n",
    "def query_inverted_index_sorted_by_word_counts_top_n(query, n):\n",
    "    with shelve.open(INVERTED_INDEX_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique(query)]\n",
    "        result = list()\n",
    "        for term in terms:\n",
    "            if not term in db:\n",
    "                continue\n",
    "            result.extend(db[term])\n",
    "        return Counter(result).most_common(n)\n",
    "    \n",
    "def evaluate_inverted_index(query_files, runID, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True) \n",
    "    for filename in query_files:\n",
    "        print(filename)\n",
    "        print(datetime.now())\n",
    "        with open(output_path + \"/\" + filename.split(\"/\")[-1].split(\".\")[0] + \".run\", \"w\") as output_file:\n",
    "            testdata = pd.read_json(filename, lines = True)\n",
    "            for index, row in testdata.iterrows():\n",
    "                result = query_inverted_index_sorted_by_word_counts_top_n(row[\"query\"], 1000)\n",
    "                rank_counter = 1\n",
    "                for id, frequency in result:\n",
    "                    output_file.write(f\"{row.get('query_id')} Q0 {id} {rank_counter} {frequency} {runID}\\n\")\n",
    "                    # print(f\"{row.get(\"query_id\")} Q0 {id} {rank_counter} {frequency} {runID}\")\n",
    "                    rank_counter += 1\n",
    "                \n",
    "                \n",
    "\n",
    "evaluate_inverted_index(query_files, 1, OUT_PATH + \"/inverted_index\")\n",
    "# 2 mins :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be1f53",
   "metadata": {},
   "source": [
    "## TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb95c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_unique_list(text):\n",
    "    tokens = list(word_tokenize(text.lower())) # get all tokens\n",
    "    return [i for i in tokens if i not in stop] # get all tokens without stopwords\n",
    "\n",
    "\n",
    "def query_tf_idf_top_n(query, n):\n",
    "    with shelve.open(TF_IDF_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique_list(query)]\n",
    "        \n",
    "        tf_idf_sums = defaultdict(float)\n",
    "        \n",
    "        for term in terms:\n",
    "            if term not in db:\n",
    "                continue\n",
    "            data = db[term]\n",
    "            idf = data[\"idf\"]\n",
    "            doc_ids = data[\"doc_ids\"]\n",
    "            tfs = data[\"tfs\"]\n",
    "\n",
    "            # in the database the tfs for each document is stored for the documents index in doc_ids, for this reason we can easily zip them together :)\n",
    "            for doc_id, tf in zip(doc_ids, tfs):\n",
    "                tf_idf_sums[doc_id] +=  tf * idf\n",
    "        \n",
    "        return sorted(tf_idf_sums.items(), key = lambda x: x[1], reverse = True)[:n]\n",
    "\n",
    "def evaluate_tf_idf(query_files, runID, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for filename in query_files:\n",
    "        print(filename)\n",
    "        print(datetime.now())\n",
    "        with open(output_path + \"/\" + filename.split(\"/\")[-1].split(\".\")[0] + \".run\", \"w\") as output_file:\n",
    "            testdata = pd.read_json(filename, lines = True)\n",
    "            for index, row in testdata.iterrows():\n",
    "                result = query_tf_idf_top_n(row[\"query\"], 1000)\n",
    "                rank_counter = 1\n",
    "                for id, tf_idf in result:\n",
    "                    # output_file.write(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\\n\")\n",
    "                    print(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\")\n",
    "                    rank_counter += 1\n",
    "            \n",
    "evaluate_tf_idf(query_files, 1, OUT_PATH + \"/tf_idf\")\n",
    "# 3.5 mins :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f539eb",
   "metadata": {},
   "source": [
    "# Reduced queries without weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4057616",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = \"../data/runs/Reduced_Query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e87f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists and is a file\n",
      "id                                                        153\n",
      "keywords    [{'term': 'martial arts', 'weight': 1.0}, {'te...\n",
      "guesses                                                    []\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_unique_list(text):\n",
    "    tokens = list(word_tokenize(text.lower())) # get all tokens\n",
    "    return [i for i in tokens if i not in stop] # get all tokens without stopwords\n",
    "\n",
    "\n",
    "for filename in query_files:\n",
    "    filename_improved_query = \"./data/improved_queries/\" + re.search(r'([^/]+)(?=\\.[^.]+$)', filename).group(1) + \"_improved_queries.jsonl\"\n",
    "    if os.path.isfile(filename_improved_query):\n",
    "        print(\"File exists and is a file\")\n",
    "    else:\n",
    "        print(\"No file found\")\n",
    "    df_improved_queries = pd.read_json(filename_improved_query, lines=True)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f552725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/tot25/subsets/eval20/eval20-queries-dev1.jsonl\n",
      "2025-11-24 11:16:35.700753\n",
      "152 Q0 71362650 1 3203.3987519361317 1\n",
      "153 Q0 213472 1 1587.3387238090706 1\n",
      "185 Q0 74998017 1 2877.841379854192 1\n",
      "240 Q0 101888 1 2724.5155889333832 1\n",
      "365 Q0 3883945 1 1877.3104658814264 1\n",
      "385 Q0 13406737 1 2327.1467034988177 1\n",
      "391 Q0 4035 1 1087.6573675854895 1\n",
      "403 Q0 294286 1 3247.5551396219225 1\n",
      "442 Q0 74998017 1 5071.727830631169 1\n",
      "458 Q0 13763276 1 2255.6470977564754 1\n",
      "490 Q0 3333003 1 2607.1318156824786 1\n",
      "498 Q0 21013155 1 1920.6357716685457 1\n",
      "512 Q0 57197 1 20031.792468411513 1\n",
      "554 Q0 74998017 1 3530.652500502496 1\n",
      "563 Q0 26349927 1 1654.1196354539575 1\n",
      "625 Q0 28356988 1 3714.103977030983 1\n",
      "639 Q0 74998017 1 2403.1170172844 1\n",
      "722 Q0 314828 1 1677.024229501376 1\n",
      "735 Q0 74998017 1 3107.0308366366835 1\n",
      "761 Q0 54090522 1 3739.2825552375775 1\n",
      "766 Q0 1303939 1 2179.0423352831876 1\n",
      "786 Q0 57197 1 1574.6641012070495 1\n",
      "813 Q0 21013155 1 1796.8114057738665 1\n",
      "850 Q0 74998017 1 2307.337436065065 1\n",
      "861 Q0 8557881 1 1970.6483197353427 1\n",
      "898 Q0 74998017 1 2935.534114390452 1\n",
      "905 Q0 66298477 1 1255.4845738989884 1\n",
      "919 Q0 74998017 1 2828.660616981044 1\n",
      "1011 Q0 74998017 1 2648.2322045935734 1\n",
      "../data/tot25/subsets/eval20/eval20-queries-dev2.jsonl\n",
      "2025-11-24 11:16:40.341168\n",
      "108 Q0 67482183 1 4579.039356845806 1\n",
      "126 Q0 5338312 1 2810.957352676204 1\n",
      "162 Q0 959699 1 2940.663063035399 1\n",
      "200 Q0 66326874 1 2156.0791128357278 1\n",
      "243 Q0 66326874 1 2089.8898904889 1\n",
      "253 Q0 111582 1 1616.2724156265388 1\n",
      "261 Q0 2585985 1 1348.825656796925 1\n",
      "280 Q0 959699 1 3020.212214137138 1\n",
      "297 Q0 5970937 1 1651.3579797781351 1\n",
      "361 Q0 23487440 1 2331.541780040364 1\n",
      "467 Q0 26888898 1 4155.858800792129 1\n",
      "516 Q0 1794747 1 1842.9561628441318 1\n",
      "528 Q0 11843604 1 1118.4032780063383 1\n",
      "539 Q0 71362650 1 1913.2183454583187 1\n",
      "557 Q0 57197 1 4719.3577920056105 1\n",
      "558 Q0 74998017 1 2139.3783143006685 1\n",
      "582 Q0 75225 1 2094.772491047 1\n",
      "587 Q0 59999602 1 1576.8564419466036 1\n",
      "599 Q0 21013155 1 1133.2495494468878 1\n",
      "610 Q0 88857 1 3517.61940393237 1\n",
      "616 Q0 71362650 1 3284.918021055816 1\n",
      "654 Q0 18951293 1 844.5051731099898 1\n",
      "667 Q0 71362650 1 3001.3267917378685 1\n",
      "673 Q0 13763276 1 1183.4590667753857 1\n",
      "745 Q0 67482183 1 1822.2612076393962 1\n",
      "820 Q0 71362650 1 1847.751175702088 1\n",
      "964 Q0 74998017 1 4879.068946739924 1\n",
      "980 Q0 302808 1 2221.8008875394426 1\n",
      "984 Q0 294286 1 5618.257934769651 1\n",
      "../data/tot25/subsets/eval20/eval20-queries-dev3.jsonl\n",
      "2025-11-24 11:16:43.993403\n",
      "2004 Q0 186300 1 2556.99900201799 1\n",
      "2019 Q0 5791028 1 1929.7893879932183 1\n",
      "2021 Q0 74998017 1 1260.1207506937926 1\n",
      "2022 Q0 16456557 1 4384.403526144919 1\n",
      "2027 Q0 55294431 1 4779.192050271296 1\n",
      "2032 Q0 1494052 1 2007.7313868534723 1\n",
      "2033 Q0 74998017 1 2103.477874234123 1\n",
      "2038 Q0 57197 1 25006.337469283833 1\n",
      "2039 Q0 57197 1 11295.549681046381 1\n",
      "2040 Q0 28356988 1 2866.2642708181324 1\n",
      "2051 Q0 1607189 1 1874.1009440691942 1\n",
      "2059 Q0 57197 1 11165.209944161106 1\n",
      "2065 Q0 66326874 1 2111.538011323797 1\n",
      "2067 Q0 8557881 1 2511.026727117766 1\n",
      "2068 Q0 71362650 1 4727.214531053949 1\n",
      "2079 Q0 5979532 1 1673.1416044573225 1\n",
      "2080 Q0 332727 1 2084.9086400930646 1\n",
      "2081 Q0 8557881 1 1859.7806998162225 1\n",
      "2083 Q0 1939702 1 3419.407936429472 1\n",
      "2090 Q0 7206501 1 1030.2991234184904 1\n",
      "2093 Q0 67482183 1 2544.2670745378305 1\n",
      "2111 Q0 43652689 1 1766.3739783162503 1\n",
      "2113 Q0 21013155 1 1476.4779749101815 1\n",
      "2116 Q0 8557881 1 1973.0358730391267 1\n",
      "2117 Q0 235869 1 1555.1619681173445 1\n",
      "2121 Q0 52020491 1 1964.1781943149588 1\n",
      "2127 Q0 2642 1 1558.773864685607 1\n",
      "2128 Q0 235869 1 2806.7934842239088 1\n",
      "2134 Q0 53353 1 3076.408487584874 1\n",
      "2139 Q0 33340052 1 1099.8860461487461 1\n",
      "2155 Q0 71362650 1 1915.8979025905537 1\n",
      "2172 Q0 58902863 1 1173.7380784030195 1\n",
      "2173 Q0 2639875 1 1340.2332183762321 1\n",
      "2174 Q0 2561200 1 1520.8126579034417 1\n",
      "2176 Q0 67482183 1 2132.9491488616695 1\n",
      "2186 Q0 35068593 1 899.7542575374074 1\n",
      "2188 Q0 3657333 1 1733.3650301503885 1\n",
      "2191 Q0 53353 1 1811.244855534417 1\n",
      "2216 Q0 69919 1 648.5339471320157 1\n",
      "2221 Q0 302808 1 1121.4068454676885 1\n",
      "2227 Q0 71310604 1 658.5118293801164 1\n",
      "2230 Q0 8557881 1 1847.7619920020306 1\n",
      "2244 Q0 8210131 1 1974.6459886484815 1\n",
      "2245 Q0 6441212 1 1479.941920477245 1\n",
      "2249 Q0 92536 1 820.8599128389146 1\n",
      "2257 Q0 33257452 1 1090.8980686624577 1\n",
      "2259 Q0 24711516 1 1472.1983650838772 1\n",
      "2265 Q0 1590884 1 1849.3423154292652 1\n",
      "2272 Q0 26888898 1 2185.781300287726 1\n",
      "2273 Q0 3150028 1 940.6327750437018 1\n",
      "2274 Q0 6754695 1 1238.2505244627957 1\n",
      "2280 Q0 63298428 1 807.7513133369364 1\n",
      "2284 Q0 8938540 1 911.5982909539308 1\n",
      "2296 Q0 39512776 1 1214.0665236125394 1\n",
      "2298 Q0 302808 1 2026.695162261631 1\n",
      "2322 Q0 406051 1 3800.9977190434006 1\n",
      "2334 Q0 54630 1 2126.1279626811406 1\n",
      "2336 Q0 8557881 1 956.6560350947292 1\n",
      "2337 Q0 71362650 1 1576.56399138027 1\n",
      "2338 Q0 26941571 1 1350.7097706700995 1\n",
      "2341 Q0 43652689 1 1552.3957545252176 1\n",
      "2350 Q0 71362650 1 1469.8802821381016 1\n",
      "2356 Q0 21013155 1 1375.1825348296713 1\n",
      "2372 Q0 6083353 1 558.6863954618533 1\n",
      "2377 Q0 46979246 1 3551.0396468429917 1\n",
      "2378 Q0 3008018 1 1827.5380470869484 1\n",
      "2381 Q0 57197 1 13333.810224398103 1\n",
      "2388 Q0 74998017 1 2483.892863691027 1\n",
      "2390 Q0 1797071 1 1940.5080343436648 1\n",
      "2392 Q0 19872006 1 2572.20550556912 1\n",
      "2400 Q0 67482183 1 2067.274125629203 1\n",
      "2402 Q0 2994351 1 1192.27094603775 1\n",
      "2404 Q0 2497446 1 1464.9246364603878 1\n",
      "2409 Q0 70847025 1 1440.035520372313 1\n",
      "2412 Q0 74998017 1 5421.796785927184 1\n",
      "2415 Q0 16921969 1 2251.8856635359766 1\n",
      "2418 Q0 153065 1 2196.253292661542 1\n",
      "2419 Q0 8557881 1 2650.073217054244 1\n",
      "2427 Q0 1579280 1 1720.2991882309482 1\n",
      "2439 Q0 57197 1 1998.356062944579 1\n",
      "2450 Q0 74998017 1 3796.065212045405 1\n",
      "2451 Q0 2150841 1 2708.4489501552143 1\n",
      "2455 Q0 71362650 1 1721.281609169084 1\n",
      "2459 Q0 6946727 1 2751.364720059965 1\n",
      "2460 Q0 67482183 1 2092.058650002864 1\n",
      "2479 Q0 426208 1 1655.785728399594 1\n",
      "2482 Q0 3395 1 2063.0659626572487 1\n",
      "2483 Q0 1388019 1 1514.7563723815401 1\n",
      "2489 Q0 210728 1 1228.6340424642253 1\n",
      "2491 Q0 164910 1 3058.290236291705 1\n",
      "2498 Q0 276879 1 917.6872495586024 1\n",
      "2516 Q0 28356988 1 1825.0776593492615 1\n",
      "2521 Q0 959699 1 2830.6150874868495 1\n",
      "2523 Q0 7412236 1 1405.2724787325492 1\n",
      "2524 Q0 71362650 1 1767.7811668912507 1\n",
      "2530 Q0 142340 1 1367.2246425793924 1\n",
      "2540 Q0 4724183 1 2488.9799774172075 1\n",
      "2558 Q0 18720509 1 2750.4050278065065 1\n",
      "2574 Q0 68187 1 3904.4573466210686 1\n",
      "2578 Q0 71362650 1 3781.173653028026 1\n",
      "2582 Q0 13763276 1 1609.5400642931647 1\n",
      "2583 Q0 13200141 1 1168.130645293529 1\n",
      "2585 Q0 52020491 1 1190.083471658346 1\n",
      "2590 Q0 2150841 1 2825.184030360224 1\n",
      "2595 Q0 57197 1 13733.156920958167 1\n",
      "2596 Q0 67482183 1 4013.160102344015 1\n",
      "2598 Q0 959699 1 2740.3456314389405 1\n",
      "2600 Q0 21013155 1 1247.0788923793316 1\n",
      "../data/tot25/subsets/eval20/eval20-queries-train.jsonl\n",
      "2025-11-24 11:16:54.834361\n",
      "116 Q0 21506072 1 1026.4294945319784 1\n",
      "119 Q0 6754695 1 2388.935030066088 1\n",
      "125 Q0 111582 1 1804.8520684473567 1\n",
      "147 Q0 29206809 1 1457.794394439779 1\n",
      "204 Q0 7893660 1 1172.7141148600022 1\n",
      "239 Q0 74998017 1 2179.4418504057853 1\n",
      "291 Q0 111582 1 838.2242801659527 1\n",
      "352 Q0 566000 1 1656.0237174907656 1\n",
      "354 Q0 6754695 1 925.2941910561079 1\n",
      "384 Q0 70823343 1 2964.163043519679 1\n",
      "588 Q0 74998017 1 2188.8784095090264 1\n",
      "600 Q0 23487440 1 2344.342233867182 1\n",
      "705 Q0 74998017 1 2058.899201599144 1\n",
      "727 Q0 5577654 1 1263.96254583725 1\n",
      "731 Q0 635253 1 1117.359296839124 1\n",
      "741 Q0 4059850 1 1662.7563270844232 1\n",
      "784 Q0 8024324 1 1824.6041243156337 1\n",
      "796 Q0 54465810 1 1373.3008965146498 1\n",
      "801 Q0 11843604 1 1712.1981363516284 1\n",
      "818 Q0 187637 1 3898.704724730816 1\n",
      "823 Q0 5099311 1 953.8151305165899 1\n",
      "829 Q0 74998017 1 2038.4181686262368 1\n",
      "838 Q0 294286 1 5726.7782457032445 1\n",
      "937 Q0 23487440 1 2050.3917088871026 1\n",
      "1007 Q0 7222823 1 1254.2546902247566 1\n",
      "1021 Q0 51910653 1 1071.9003286653501 1\n",
      "1042 Q0 57197 1 1220.5689507878424 1\n",
      "1048 Q0 74998017 1 2703.1016745397246 1\n",
      "1050 Q0 57197 1 650.1098404341659 1\n"
     ]
    }
   ],
   "source": [
    "def query_tf_idf_top_n(query, n):\n",
    "    with shelve.open(TF_IDF_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique_list(query)]\n",
    "        \n",
    "        tf_idf_sums = defaultdict(float)\n",
    "        \n",
    "        for term in terms:\n",
    "            if term not in db:\n",
    "                continue\n",
    "            data = db[term]\n",
    "            idf = data[\"idf\"]\n",
    "            doc_ids = data[\"doc_ids\"]\n",
    "            tfs = data[\"tfs\"]\n",
    "\n",
    "            # in the database the tfs for each document is stored for the documents index in doc_ids, for this reason we can easily zip them together :)\n",
    "            for doc_id, tf in zip(doc_ids, tfs):\n",
    "                tf_idf_sums[doc_id] +=  tf * idf\n",
    "        \n",
    "        return sorted(tf_idf_sums.items(), key = lambda x: x[1], reverse = True)[:n]\n",
    "\n",
    "def evaluate_tf_idf_reduced_query(query_files, runID, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for filename in query_files:\n",
    "        print(filename)\n",
    "        print(datetime.now())\n",
    "        filename_improved_query = \"./data/improved_queries/\" + re.search(r'([^/]+)(?=\\.[^.]+$)', filename).group(1) + \"_improved_queries.jsonl\"\n",
    "        df_improved_queries = pd.read_json(filename_improved_query, lines = True)\n",
    "        df_improved_queries[\"terms_string\"] = df_improved_queries[\"keywords\"].apply(lambda items: \" \".join([d[\"term\"] for d in items]))\n",
    "        \n",
    "        \n",
    "        with open(output_path + \"/\" + filename.split(\"/\")[-1].split(\".\")[0] + \".run\", \"w\") as output_file:\n",
    "            testdata = pd.read_json(filename, lines = True)\n",
    "            for index, row in testdata.iterrows():\n",
    "                result = query_tf_idf_top_n(df_improved_queries[df_improved_queries[\"id\"] == row.get(\"query_id\")][\"terms_string\"].iloc[0], 1000)\n",
    "                rank_counter = 1\n",
    "                for id, tf_idf in result:\n",
    "                    # output_file.write(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\\n\")\n",
    "                    print(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\")\n",
    "                    rank_counter += 1\n",
    "                    break\n",
    "            \n",
    "evaluate_tf_idf_reduced_query(query_files, 1, OUT_PATH + \"/tf_idf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
