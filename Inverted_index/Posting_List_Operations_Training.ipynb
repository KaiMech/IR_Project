{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fceeb238",
   "metadata": {},
   "source": [
    "# Posting List operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440fc3f",
   "metadata": {},
   "source": [
    "This notebook is to evaluate inverted index and tf_idf on any given datasets.\n",
    "For usage adapt the last 4 variables in the first cell (inverted_index_path, tf_idf_path, out_path and query_files) accordingly, depending on whether everything should be executed fo train or eval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f751290",
   "metadata": {},
   "source": [
    "todos:\n",
    "-   review everything\n",
    "-   run for train and test set\n",
    "-   check output file parameter if rank -> tf_idf is correct or this is meant elsewise --> Ciwan confirmed is fine, must just be sorted :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5176647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import shelve\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math \n",
    "from collections import Counter\n",
    "from collections import defaultdict \n",
    "from datetime import datetime\n",
    "import re\n",
    "import math \n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    import orjson as _json\n",
    "    loads = _json.loads\n",
    "except Exception:\n",
    "    import json as _json\n",
    "    loads = _json.loads\n",
    "\n",
    "stop = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "INVERTED_INDEX_PATH = \"./data/inverted_index_eval.db\"\n",
    "TF_IDF_PATH = \"./data/tf_idf_eval.db\"\n",
    "OUT_PATH = \"../data/runs\"\n",
    "query_files = [\"../data/tot25/subsets/eval20/eval20-queries-dev1.jsonl\", \"../data/tot25/subsets/eval20/eval20-queries-dev2.jsonl\", \"../data/tot25/subsets/eval20/eval20-queries-dev3.jsonl\", \"../data/tot25/subsets/eval20/eval20-queries-train.jsonl\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e6477",
   "metadata": {},
   "source": [
    "## Function for Lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "499def07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Kai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'notebook', 'be', 'use', 'to', 'create', 'an', 'inverted_index', 'and', 'tf_idf', 'database', 'for', 'the', 'provided', 'corpus', 'file', '(', 'train', 'or', 'eval', ')', '.', 'adapt', 'the', 'variable', 'in', 'the', 'first', 'cell', 'accordingly', 'for', 'file', 'and', 'output', 'dir.note', 'that', 'the', 'executino', 'time', 'for', 'eval', 'be', '1/3', 'that', 'of', 'train', ',', 'despite', 'the', 'fact', 'that', 'eval', 'contain', '2.5', 'time', 'the', 'amount', 'of', 'doc', '.', 'we', 'assume', 'this', 'be']\n"
     ]
    }
   ],
   "source": [
    "# download for lematization\n",
    "nltk.download('punkt_tab')   \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def get_wordnet_pos(tag): # mapping POS tag from pos_tag to a format WordNetLemmatizer accepts.\n",
    "    match tag[0]:\n",
    "        case 'J':\n",
    "            return wordnet.ADJ\n",
    "        case 'V':\n",
    "            return wordnet.VERB\n",
    "        #case 'N':\n",
    "        #    return wordnet.NOUN       online source: here, but twice no benefit so removed\n",
    "        case 'R':\n",
    "            return wordnet.ADV\n",
    "        case _:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lematization(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    return [lemmatizer.lemmatize(token, get_wordnet_pos(tag)).lower() for token, tag in tagged] # lower so can compare eg write friday instead of Friday\n",
    "\n",
    "#sanity check\n",
    "lem = lematization(\"This notebook is used to create an inverted_index and tf_idf database for the provided corpus file (train or eval). Adapt the variables in the first cell accordingly for files and output dir.Note that the executino time for eval is 1/3 that of train, despite the fact that eval contains 2.5 times the amount of docs. We assume this is \")\n",
    "print(lem) # later need to remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b7850e",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14994e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and preprocess a document\n",
    "def preprocess_unique(text):\n",
    "    #old function changed to lematization    tokens = set(word_tokenize(text.lower())) # get all tokens\n",
    "    tokens = set(lematization(text))\n",
    "    return [i for i in tokens if i not in stop] # get all tokens without stopwords\n",
    "\n",
    "def AND(l1, l2):\n",
    "    # note: this method assumes that both lists are sorted and do not contain any duplicates (which if the input comes from a posting list is given)\n",
    "    # this asssumption is given considering the inverted index and tf_idf db's were created\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    sqrtJump1 = max(math.isqrt(len(l1)), 1)\n",
    "    sqrtJump2 = max(math.isqrt(len(l2)), 1)\n",
    "    result = []\n",
    "    while p1 < len(l1)  and p2 < len(l2):\n",
    "        if l1[p1] == l2[p2]:\n",
    "            result.append(l1[p1])\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "        # skip pointer in first list\n",
    "        elif p1 % sqrtJump1 == 0 and p1 + sqrtJump1 < len(l1) and l1[p1 + sqrtJump1] <= l2[p2]:\n",
    "            p1 += sqrtJump1\n",
    "        # skip pointer in second list\n",
    "        elif p2 % sqrtJump2 == 0 and p2 + sqrtJump2 < len(l2) and l2[p2 + sqrtJump2] <= l1[p1]:\n",
    "            p2 += sqrtJump2\n",
    "        elif l1[p1] < l2[p2]:\n",
    "            p1 += 1\n",
    "        else:\n",
    "            p2 += 1\n",
    "    return result\n",
    "\n",
    "def OR(l1, l2):\n",
    "    # note: this method assumes that both lists are sorted and do not contain any duplicates (which if the input comes from a posting list is given)\n",
    "    # this asssumption is given considering the inverted index and tf_idf db's were created\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    result = []\n",
    "    while p1 < len(l1) and p2 < len(l2):\n",
    "        if l1[p1] < l2[p2]:\n",
    "            result.append(l1[p1])\n",
    "            p1 += 1\n",
    "        elif l1[p1] > l2[p2]:\n",
    "            result.append(l2[p2])\n",
    "            p2 += 1\n",
    "        else: \n",
    "            result.append(l1[p1])\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "    while p1 < len(l1):\n",
    "        result.append(l1[p1])\n",
    "        p1 += 1\n",
    "    while p2 < len(l2):\n",
    "        result.append(l2[p2])\n",
    "        p2 += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af8353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/tot25/subsets/eval20/eval20-queries-dev1.jsonl\n",
      "2025-11-25 10:32:17.494569\n",
      "../data/tot25/subsets/eval20/eval20-queries-dev2.jsonl\n",
      "2025-11-25 10:32:23.910188\n",
      "../data/tot25/subsets/eval20/eval20-queries-dev3.jsonl\n",
      "2025-11-25 10:32:30.052930\n",
      "../data/tot25/subsets/eval20/eval20-queries-train.jsonl\n",
      "2025-11-25 10:33:08.426017\n"
     ]
    }
   ],
   "source": [
    "def query_inverted_index_and(query):\n",
    "    with shelve.open(INVERTED_INDEX_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique(query)]\n",
    "        terms.sort(key = lambda t:len(db[t]))\n",
    "        result = db[terms[0]]\n",
    "        for term in terms[1:]:\n",
    "            result = AND(result, db[term])\n",
    "        return result\n",
    "\n",
    "def query_inverted_index_sorted_by_word_counts_top_n(query, n):\n",
    "    with shelve.open(INVERTED_INDEX_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique(query)]\n",
    "        result = list()\n",
    "        for term in terms:\n",
    "            if not term in db:\n",
    "                continue\n",
    "            result.extend(db[term])\n",
    "        return Counter(result).most_common(n)\n",
    "    \n",
    "def evaluate_inverted_index(query_files, runID, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True) \n",
    "    for filename in query_files:\n",
    "        print(filename)\n",
    "        print(datetime.now())\n",
    "        with open(output_path + \"/\" + filename.split(\"/\")[-1].split(\".\")[0] + \".run\", \"w\") as output_file:\n",
    "            testdata = pd.read_json(filename, lines = True)\n",
    "            for index, row in testdata.iterrows():\n",
    "                result = query_inverted_index_sorted_by_word_counts_top_n(row[\"query\"], 1000)\n",
    "                rank_counter = 1\n",
    "                for id, frequency in result:\n",
    "                    output_file.write(f\"{row.get('query_id')} Q0 {id} {rank_counter} {frequency} {runID}\\n\")\n",
    "                    # print(f\"{row.get(\"query_id\")} Q0 {id} {rank_counter} {frequency} {runID}\")\n",
    "                    rank_counter += 1\n",
    "                \n",
    "                \n",
    "\n",
    "evaluate_inverted_index(query_files, 1, OUT_PATH + \"/inverted_index\")\n",
    "# 1 min :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be1f53",
   "metadata": {},
   "source": [
    "## TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb95c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/tot25/subsets/eval20/eval20-queries-dev1.jsonl\n",
      "2025-11-25 10:36:04.016110\n",
      "../data/tot25/subsets/eval20/eval20-queries-dev2.jsonl\n",
      "2025-11-25 10:36:23.149638\n",
      "../data/tot25/subsets/eval20/eval20-queries-dev3.jsonl\n",
      "2025-11-25 10:36:40.632113\n",
      "../data/tot25/subsets/eval20/eval20-queries-train.jsonl\n",
      "2025-11-25 10:38:35.481766\n"
     ]
    }
   ],
   "source": [
    "def preprocess_unique_list(text):\n",
    "    #old function changed to lematization    tokens = list(word_tokenize(text.lower())) # get all tokens\n",
    "    tokens = list(lematization(text)) # get all tokens\n",
    "    return [i for i in tokens if i not in stop] # get all tokens without stopwords\n",
    "\n",
    "\n",
    "def query_tf_idf_top_n(query, n):\n",
    "    with shelve.open(TF_IDF_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique_list(query)]\n",
    "        \n",
    "        tf_idf_sums = defaultdict(float)\n",
    "        \n",
    "        for term in terms:\n",
    "            if term not in db:\n",
    "                continue\n",
    "            data = db[term]\n",
    "            idf = data[\"idf\"]\n",
    "            doc_ids = data[\"doc_ids\"]\n",
    "            tfs = data[\"tfs\"]\n",
    "\n",
    "            # in the database the tfs for each document is stored for the documents index in doc_ids, for this reason we can easily zip them together :)\n",
    "            for doc_id, tf in zip(doc_ids, tfs):\n",
    "                tf_idf_sums[doc_id] +=  math.log(tf + 1) * idf\n",
    "        \n",
    "        return sorted(tf_idf_sums.items(), key = lambda x: x[1], reverse = True)[:n]\n",
    "\n",
    "def evaluate_tf_idf(query_files, runID, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for filename in query_files:\n",
    "        print(filename)\n",
    "        print(datetime.now())\n",
    "        with open(output_path + \"/\" + filename.split(\"/\")[-1].split(\".\")[0] + \".run\", \"w\") as output_file:\n",
    "            testdata = pd.read_json(filename, lines = True)\n",
    "            for index, row in testdata.iterrows():\n",
    "                result = query_tf_idf_top_n(row[\"query\"], 1000)\n",
    "                rank_counter = 1\n",
    "                for id, tf_idf in result:\n",
    "                    output_file.write(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\\n\")\n",
    "                    # print(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\")\n",
    "                    rank_counter += 1\n",
    "            \n",
    "evaluate_tf_idf(query_files, 1, OUT_PATH + \"/tf_idf\")\n",
    "# 2:49 mins :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f539eb",
   "metadata": {},
   "source": [
    "# Reduced queries without weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4057616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'be', 'sing']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_unique_list(text):\n",
    "    #old function changed to lematization    tokens = list(word_tokenize(text.lower())) # get all tokens\n",
    "    tokens = list(lematization(text)) # get all tokens\n",
    "    return [i for i in tokens if i not in stop] # get all tokens without stopwords\n",
    "\n",
    "tokens = list(lematization(\"I am singing\"))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f552725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/tot25/subsets/eval20/eval20-queries-dev1.jsonl\n",
      "2025-11-25 10:39:51.421430\n",
      "../data/tot25/subsets/eval20/eval20-queries-dev2.jsonl\n",
      "2025-11-25 10:39:56.836661\n",
      "../data/tot25/subsets/eval20/eval20-queries-dev3.jsonl\n",
      "2025-11-25 10:40:01.444993\n",
      "../data/tot25/subsets/eval20/eval20-queries-train.jsonl\n",
      "2025-11-25 10:40:16.896775\n"
     ]
    }
   ],
   "source": [
    "def query_tf_idf_top_n(query, n):\n",
    "    with shelve.open(TF_IDF_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique_list(query)]\n",
    "        \n",
    "        tf_idf_sums = defaultdict(float)\n",
    "        \n",
    "        for term in terms:\n",
    "            if term not in db:\n",
    "                continue\n",
    "            data = db[term]\n",
    "            idf = data[\"idf\"]\n",
    "            doc_ids = data[\"doc_ids\"]\n",
    "            tfs = data[\"tfs\"]\n",
    "\n",
    "            # in the database the tfs for each document is stored for the documents index in doc_ids, for this reason we can easily zip them together :)\n",
    "            for doc_id, tf in zip(doc_ids, tfs):\n",
    "                tf_idf_sums[doc_id] +=  math.log(tf + 1) * idf\n",
    "        \n",
    "        return sorted(tf_idf_sums.items(), key = lambda x: x[1], reverse = True)[:n]\n",
    "\n",
    "def evaluate_tf_idf_reduced_query(query_files, runID, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for filename in query_files:\n",
    "        print(filename)\n",
    "        print(datetime.now())\n",
    "        filename_improved_query = \"./data/improved_queries/\" + re.search(r'([^/]+)(?=\\.[^.]+$)', filename).group(1) + \"_improved_queries.jsonl\"\n",
    "        df_improved_queries = pd.read_json(filename_improved_query, lines = True)\n",
    "        df_improved_queries[\"terms_string\"] = df_improved_queries[\"keywords\"].apply(lambda items: \" \".join([d[\"term\"] for d in items]))\n",
    "        \n",
    "        \n",
    "        with open(output_path + \"/\" + filename.split(\"/\")[-1].split(\".\")[0] + \".run\", \"w\") as output_file:\n",
    "            testdata = pd.read_json(filename, lines = True)\n",
    "            for index, row in testdata.iterrows():\n",
    "                result = query_tf_idf_top_n(df_improved_queries[df_improved_queries[\"id\"] == row.get(\"query_id\")][\"terms_string\"].iloc[0], 1000)\n",
    "                rank_counter = 1\n",
    "                for id, tf_idf in result:\n",
    "                    output_file.write(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\\n\")\n",
    "                    # print(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\")\n",
    "                    rank_counter += 1\n",
    "            \n",
    "evaluate_tf_idf_reduced_query(query_files, 1, OUT_PATH + \"/tf_idf_reduced_queries\")\n",
    "# 30 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d376b",
   "metadata": {},
   "source": [
    "# Distance Weighted retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c7a37",
   "metadata": {},
   "source": [
    "In this section we use TF-IDF but in addition weight the query terms by their index, i.e. first word gets weight 1, last gets minimum weight. \n",
    "The weighting function is \n",
    "1 - (1 - MIN_WEIGHT) * (index/ query_length)^2 \n",
    "\n",
    "We chose this function for the following reasons:\n",
    "1. Weights should be in the range of 1 max and MIN_WEIGHT = 0.4 min (minimum value worked the best with experience)\n",
    "2. The function should be concave and decreasing slowly, hence the ^2\n",
    "3. The weight should not only depend on the index, but also on the length of the query, i.e. first Token gets weight 1, last gets weight MIN_WEIGHT, no matter how long the query is. That is because our query lengths range from tens to hundreds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ceb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/tot25/subsets/eval20/eval20-queries-dev1.jsonl\n",
      "2025-11-25 10:41:47.004529\n",
      "../data/tot25/subsets/eval20/eval20-queries-dev2.jsonl\n",
      "2025-11-25 10:41:54.355193\n",
      "../data/tot25/subsets/eval20/eval20-queries-dev3.jsonl\n",
      "2025-11-25 10:42:00.804552\n",
      "../data/tot25/subsets/eval20/eval20-queries-train.jsonl\n",
      "2025-11-25 10:42:20.790891\n"
     ]
    }
   ],
   "source": [
    "MIN_WEIGHT = 0.4 \n",
    "def query_tf_idf_top_n(query, n):\n",
    "    with shelve.open(TF_IDF_PATH) as db:\n",
    "        terms = [t for t in preprocess_unique_list(query)]\n",
    "        term_count = len(terms)\n",
    "        \n",
    "        tf_idf_sums = defaultdict(float)\n",
    "        \n",
    "        counter = 0\n",
    "        \n",
    "        for term in terms:\n",
    "            if term not in db:\n",
    "                continue\n",
    "            data = db[term]\n",
    "            idf = data[\"idf\"]\n",
    "            doc_ids = data[\"doc_ids\"]\n",
    "            tfs = data[\"tfs\"]\n",
    "\n",
    "            # in the database the tfs for each document is stored for the documents index in doc_ids, for this reason we can easily zip them together :)\n",
    "            for doc_id, tf in zip(doc_ids, tfs):\n",
    "                tf_idf_sums[doc_id] +=  math.log(1 + tf) * idf * (1 - (1 - MIN_WEIGHT) * ((counter / term_count) ** 2))\n",
    "            counter += 1\n",
    "        \n",
    "        return sorted(tf_idf_sums.items(), key = lambda x: x[1], reverse = True)[:n]\n",
    "\n",
    "def evaluate_tf_idf_reduced_query(query_files, runID, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for filename in query_files:\n",
    "        print(filename)\n",
    "        print(datetime.now())\n",
    "        filename_improved_query = \"./data/improved_queries/\" + re.search(r'([^/]+)(?=\\.[^.]+$)', filename).group(1) + \"_improved_queries.jsonl\"\n",
    "        df_improved_queries = pd.read_json(filename_improved_query, lines = True)\n",
    "        df_improved_queries[\"terms_string\"] = df_improved_queries[\"keywords\"].apply(lambda items: \" \".join([d[\"term\"] for d in items]))\n",
    "        \n",
    "        \n",
    "        with open(output_path + \"/\" + filename.split(\"/\")[-1].split(\".\")[0] + \".run\", \"w\") as output_file:\n",
    "            testdata = pd.read_json(filename, lines = True)\n",
    "            for index, row in testdata.iterrows():\n",
    "                result = query_tf_idf_top_n(df_improved_queries[df_improved_queries[\"id\"] == row.get(\"query_id\")][\"terms_string\"].iloc[0], 1000)\n",
    "                rank_counter = 1\n",
    "                for id, tf_idf in result:\n",
    "                    output_file.write(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\\n\")\n",
    "                    # print(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\")\n",
    "                    rank_counter += 1\n",
    "            \n",
    "evaluate_tf_idf_reduced_query(query_files, 1, OUT_PATH + \"/tf_idf_distance_weighted\")\n",
    "# 39 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c75c5",
   "metadata": {},
   "source": [
    "# Reduced queries weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed8b3c",
   "metadata": {},
   "source": [
    "No we also account for the weights given by ChatGPT. For each proposed phrase, like \"action thriller\", we preprocess it and split it into unique terms, then compute the min tf over all terms and the maximum corresponding idf over all unique terms in the phrase, based on that we compute a weighted sum tf-idf over all proposed phrases.\n",
    "1. minimum tf: since we do not store phrases in our tf-idf, but only unique tokens, we make the assumption that the given terms in a phrase co-occur in the amount of the minimum tf  of the terms. We are aware that this might not be the case, however due to computational constraints and for storage reasons we cannot compute the tf-idf based on phrases, which is why we make this assumption.\n",
    "2. maximum idf: Since in (1) we assume that the occurence of the phrase is defined by the least occuring term in the phrase, we take the maximum idf, as this corresponds to the least occuring term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cd0e831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/tot25/subsets/eval20/eval20-queries-dev1.jsonl\n",
      "2025-11-25 11:04:18.026441\n",
      "../data/tot25/subsets/eval20/eval20-queries-dev2.jsonl\n",
      "2025-11-25 11:04:23.552340\n",
      "../data/tot25/subsets/eval20/eval20-queries-dev3.jsonl\n",
      "2025-11-25 11:04:29.467181\n",
      "../data/tot25/subsets/eval20/eval20-queries-train.jsonl\n",
      "2025-11-25 11:04:45.572321\n"
     ]
    }
   ],
   "source": [
    "def query_tf_idf_top_n(terms_weights, n):\n",
    "    with shelve.open(TF_IDF_PATH) as db:\n",
    "        tf_idf_sums = defaultdict(float) # default value is 0 :)\n",
    "        for entry in terms_weights:\n",
    "            datas = []\n",
    "            # db does not contain subterm --> ignore it for quering\n",
    "            preprocessed_split = preprocess_unique_list(entry[\"term\"])\n",
    "            if any(term not in db for term in preprocessed_split):\n",
    "                continue\n",
    "            for term in preprocessed_split:\n",
    "                datas.append(db[term])\n",
    "            if len(datas) == 0:\n",
    "                continue\n",
    "            idf = max(data[\"idf\"] for data in datas)\n",
    "            doc_ids = set.intersection(*(set(data[\"doc_ids\"]) for data in datas))\n",
    "            min_tfs = defaultdict(lambda: float(\"inf\"))\n",
    "            for data in datas:\n",
    "                for doc_id, tf in zip(data[\"doc_ids\"], data[\"tfs\"]):\n",
    "                    min_tfs[doc_id] = min(min_tfs[doc_id], tf)\n",
    "            for doc_id in doc_ids:\n",
    "                tf_idf_sums[doc_id] += math.log(min_tfs[doc_id] + 1) *  idf * entry[\"weight\"]\n",
    "                \n",
    "        return sorted(tf_idf_sums.items(), key = lambda x: x[1], reverse = True)[:n]\n",
    "   \n",
    "\n",
    "def evaluate_tf_idf_reduced_query(query_files, runID, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for filename in query_files:\n",
    "        print(filename)\n",
    "        print(datetime.now())\n",
    "        filename_improved_query = \"./data/improved_queries/\" + re.search(r'([^/]+)(?=\\.[^.]+$)', filename).group(1) + \"_improved_queries.jsonl\"\n",
    "        df_improved_queries = pd.read_json(filename_improved_query, lines = True)\n",
    "        \n",
    "        with open(output_path + \"/\" + filename.split(\"/\")[-1].split(\".\")[0] + \".run\", \"w\") as output_file:\n",
    "            testdata = pd.read_json(filename, lines = True)\n",
    "            for index, row in testdata.iterrows():\n",
    "                result = query_tf_idf_top_n(df_improved_queries[df_improved_queries[\"id\"] == row.get(\"query_id\")].iloc[0][\"keywords\"], 1000)\n",
    "                rank_counter = 1\n",
    "                for id, tf_idf in result:\n",
    "                    output_file.write(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\\n\")\n",
    "                    # print(f\"{row.get('query_id')} Q0 {id} {rank_counter} {tf_idf} {runID}\")\n",
    "                    rank_counter += 1\n",
    "\n",
    "            \n",
    "evaluate_tf_idf_reduced_query(query_files, 1, OUT_PATH + \"/tf_idf_reduced_queries_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443426e5",
   "metadata": {},
   "source": [
    "todos:\n",
    "1. run evaluation of files\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b59272",
   "metadata": {},
   "source": [
    "# Normalization by document length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee7c354b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\n",
      "Directory: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\inverted_index\n",
      "  File: eval20-queries-dev1.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "  File: eval20-queries-dev2.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "  File: eval20-queries-dev3.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "  File: eval20-queries-train.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "Directory: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf\n",
      "  File: eval20-queries-dev1.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "  File: eval20-queries-dev2.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "  File: eval20-queries-dev3.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "  File: eval20-queries-train.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "Directory: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf_distance_weighted\n",
      "  File: eval20-queries-dev1.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "  File: eval20-queries-dev2.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "  File: eval20-queries-dev3.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "  File: eval20-queries-train.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "Directory: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf_reduced_queries\n",
      "  File: eval20-queries-dev1.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "  File: eval20-queries-dev2.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "  File: eval20-queries-dev3.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "  File: eval20-queries-train.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "Directory: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf_reduced_queries_weights\n",
      "  File: eval20-queries-dev1.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "  File: eval20-queries-dev2.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "  File: eval20-queries-dev3.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n",
      "  File: eval20-queries-train.run\n",
      "\n",
      "    stderr: python: can't open file 'C:\\\\Users\\\\Kai\\\\OneDrive\\\\Vorlesungen\\\\Information Retrieval\\\\Github\\\\IR_Project\\\\Inverted_index\\\\EvalPipelineSubSet\\\\run_eval.py': [Errno 2] No such file or directory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "root_path = Path(OUT_PATH).resolve()\n",
    "\n",
    "for root, dirs, files in os.walk(root_path):\n",
    "    root = Path(root).resolve()  # absolute path\n",
    "    print(\"Directory:\", root)\n",
    "    for f in files:\n",
    "        split = f.split(\".\")[0].split(\"-\")[-1]\n",
    "        file_path = Path(root, f).resolve()  # absolute path to file\n",
    "\n",
    "        # Absolute path to the script\n",
    "        script_path = Path(\"EvalPipelineSubSet/run_eval.py\").resolve()\n",
    "\n",
    "        env = os.environ.copy()\n",
    "        env[\"PYTHONPATH\"] = str(Path(\".\").resolve())\n",
    "\n",
    "        cmd = [\n",
    "            \"python\", str(script_path),\n",
    "            \"--split\", split,\n",
    "            \"--run\", str(file_path),\n",
    "            \"--metrics\", \"ndcg@10\", \"ndcg@1000\", \"R@1000\", \"rr\", \"map\", \"map@1000\", \"P@10\", \"R@10\", \"f1@10\", \"success@10\"\n",
    "        ]\n",
    "\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, env=env)\n",
    "        print(\"  File:\", f)\n",
    "        print(result.stdout)\n",
    "        print(\"    stderr:\", result.stderr)\n",
    "        #print(\"    returncode:\", result.returncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9d9e283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: ..\\data\\runs\n",
      "  Subdirectory: ..\\data\\runs\\inverted_index\n",
      "  Subdirectory: ..\\data\\runs\\tf_idf\n",
      "  Subdirectory: ..\\data\\runs\\tf_idf_distance_weighted\n",
      "  Subdirectory: ..\\data\\runs\\tf_idf_reduced_queries\n",
      "  Subdirectory: ..\\data\\runs\\tf_idf_reduced_queries_weights\n",
      "Directory: ..\\data\\runs\\inverted_index\n",
      "    stdout: [Warning] 113 queries in qrels but not in run. Examples: ['531', '473', '659', '1095', '1038']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: dev1\n",
      "Queries (in qrels): 142\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\inverted_index\\eval20-queries-dev1.run\n",
      "     NDCG@10 : 0.000\n",
      "   NDCG@1000 : 0.002\n",
      "      R@1000 : 0.014\n",
      "          RR : 0.000\n",
      "         MAP : 0.000\n",
      "    MAP@1000 : 0.000\n",
      "        P@10 : 0.000\n",
      "        R@10 : 0.000\n",
      "       F1@10 : 0.000\n",
      "  success@10 : 0.000\n",
      "\n",
      "    stdout: [Warning] 114 queries in qrels but not in run. Examples: ['519', '1006', '477', '662', '120']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: dev2\n",
      "Queries (in qrels): 143\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\inverted_index\\eval20-queries-dev2.run\n",
      "     NDCG@10 : 0.000\n",
      "   NDCG@1000 : 0.002\n",
      "      R@1000 : 0.021\n",
      "          RR : 0.000\n",
      "         MAP : 0.000\n",
      "    MAP@1000 : 0.000\n",
      "        P@10 : 0.000\n",
      "        R@10 : 0.000\n",
      "       F1@10 : 0.000\n",
      "  success@10 : 0.000\n",
      "\n",
      "    stdout: [Warning] 428 queries in qrels but not in run. Examples: ['2001', '2002', '2003', '2005', '2006']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: dev3\n",
      "Queries (in qrels): 536\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\inverted_index\\eval20-queries-dev3.run\n",
      "     NDCG@10 : 0.003\n",
      "   NDCG@1000 : 0.009\n",
      "      R@1000 : 0.050\n",
      "          RR : 0.003\n",
      "         MAP : 0.003\n",
      "    MAP@1000 : 0.003\n",
      "        P@10 : 0.001\n",
      "        R@10 : 0.006\n",
      "       F1@10 : 0.001\n",
      "  success@10 : 0.006\n",
      "\n",
      "    stdout: [Warning] 114 queries in qrels but not in run. Examples: ['763', '802', '950', '220', '792']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: train\n",
      "Queries (in qrels): 143\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\inverted_index\\eval20-queries-train.run\n",
      "     NDCG@10 : 0.000\n",
      "   NDCG@1000 : 0.002\n",
      "      R@1000 : 0.014\n",
      "          RR : 0.000\n",
      "         MAP : 0.000\n",
      "    MAP@1000 : 0.000\n",
      "        P@10 : 0.000\n",
      "        R@10 : 0.000\n",
      "       F1@10 : 0.000\n",
      "  success@10 : 0.000\n",
      "\n",
      "Directory: ..\\data\\runs\\tf_idf\n",
      "    stdout: [Warning] 113 queries in qrels but not in run. Examples: ['531', '473', '659', '1095', '1038']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: dev1\n",
      "Queries (in qrels): 142\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf\\eval20-queries-dev1.run\n",
      "     NDCG@10 : 0.000\n",
      "   NDCG@1000 : 0.001\n",
      "      R@1000 : 0.007\n",
      "          RR : 0.000\n",
      "         MAP : 0.000\n",
      "    MAP@1000 : 0.000\n",
      "        P@10 : 0.000\n",
      "        R@10 : 0.000\n",
      "       F1@10 : 0.000\n",
      "  success@10 : 0.000\n",
      "\n",
      "    stdout: [Warning] 114 queries in qrels but not in run. Examples: ['519', '1006', '477', '662', '120']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: dev2\n",
      "Queries (in qrels): 143\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf\\eval20-queries-dev2.run\n",
      "     NDCG@10 : 0.000\n",
      "   NDCG@1000 : 0.004\n",
      "      R@1000 : 0.035\n",
      "          RR : 0.000\n",
      "         MAP : 0.000\n",
      "    MAP@1000 : 0.000\n",
      "        P@10 : 0.000\n",
      "        R@10 : 0.000\n",
      "       F1@10 : 0.000\n",
      "  success@10 : 0.000\n",
      "\n",
      "    stdout: [Warning] 428 queries in qrels but not in run. Examples: ['2001', '2002', '2003', '2005', '2006']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: dev3\n",
      "Queries (in qrels): 536\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf\\eval20-queries-dev3.run\n",
      "     NDCG@10 : 0.002\n",
      "   NDCG@1000 : 0.010\n",
      "      R@1000 : 0.056\n",
      "          RR : 0.002\n",
      "         MAP : 0.002\n",
      "    MAP@1000 : 0.002\n",
      "        P@10 : 0.000\n",
      "        R@10 : 0.004\n",
      "       F1@10 : 0.001\n",
      "  success@10 : 0.004\n",
      "\n",
      "    stdout: [Warning] 114 queries in qrels but not in run. Examples: ['763', '802', '950', '220', '792']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: train\n",
      "Queries (in qrels): 143\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf\\eval20-queries-train.run\n",
      "     NDCG@10 : 0.000\n",
      "   NDCG@1000 : 0.003\n",
      "      R@1000 : 0.021\n",
      "          RR : 0.000\n",
      "         MAP : 0.000\n",
      "    MAP@1000 : 0.000\n",
      "        P@10 : 0.000\n",
      "        R@10 : 0.000\n",
      "       F1@10 : 0.000\n",
      "  success@10 : 0.000\n",
      "\n",
      "Directory: ..\\data\\runs\\tf_idf_distance_weighted\n",
      "    stdout: [Warning] 113 queries in qrels but not in run. Examples: ['531', '473', '659', '1095', '1038']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: dev1\n",
      "Queries (in qrels): 142\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf_distance_weighted\\eval20-queries-dev1.run\n",
      "     NDCG@10 : 0.000\n",
      "   NDCG@1000 : 0.006\n",
      "      R@1000 : 0.049\n",
      "          RR : 0.000\n",
      "         MAP : 0.000\n",
      "    MAP@1000 : 0.000\n",
      "        P@10 : 0.000\n",
      "        R@10 : 0.000\n",
      "       F1@10 : 0.000\n",
      "  success@10 : 0.000\n",
      "\n",
      "    stdout: [Warning] 114 queries in qrels but not in run. Examples: ['519', '1006', '477', '662', '120']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: dev2\n",
      "Queries (in qrels): 143\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf_distance_weighted\\eval20-queries-dev2.run\n",
      "     NDCG@10 : 0.002\n",
      "   NDCG@1000 : 0.018\n",
      "      R@1000 : 0.119\n",
      "          RR : 0.002\n",
      "         MAP : 0.002\n",
      "    MAP@1000 : 0.002\n",
      "        P@10 : 0.001\n",
      "        R@10 : 0.007\n",
      "       F1@10 : 0.001\n",
      "  success@10 : 0.007\n",
      "\n",
      "    stdout: [Warning] 428 queries in qrels but not in run. Examples: ['2001', '2002', '2003', '2005', '2006']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: dev3\n",
      "Queries (in qrels): 536\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf_distance_weighted\\eval20-queries-dev3.run\n",
      "     NDCG@10 : 0.083\n",
      "   NDCG@1000 : 0.094\n",
      "      R@1000 : 0.170\n",
      "          RR : 0.078\n",
      "         MAP : 0.078\n",
      "    MAP@1000 : 0.078\n",
      "        P@10 : 0.011\n",
      "        R@10 : 0.106\n",
      "       F1@10 : 0.019\n",
      "  success@10 : 0.106\n",
      "\n",
      "    stdout: [Warning] 114 queries in qrels but not in run. Examples: ['763', '802', '950', '220', '792']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: train\n",
      "Queries (in qrels): 143\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf_distance_weighted\\eval20-queries-train.run\n",
      "     NDCG@10 : 0.000\n",
      "   NDCG@1000 : 0.011\n",
      "      R@1000 : 0.070\n",
      "          RR : 0.001\n",
      "         MAP : 0.001\n",
      "    MAP@1000 : 0.001\n",
      "        P@10 : 0.000\n",
      "        R@10 : 0.000\n",
      "       F1@10 : 0.000\n",
      "  success@10 : 0.000\n",
      "\n",
      "Directory: ..\\data\\runs\\tf_idf_reduced_queries\n",
      "    stdout: [Warning] 113 queries in qrels but not in run. Examples: ['531', '473', '659', '1095', '1038']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: dev1\n",
      "Queries (in qrels): 142\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf_reduced_queries\\eval20-queries-dev1.run\n",
      "     NDCG@10 : 0.000\n",
      "   NDCG@1000 : 0.005\n",
      "      R@1000 : 0.042\n",
      "          RR : 0.000\n",
      "         MAP : 0.000\n",
      "    MAP@1000 : 0.000\n",
      "        P@10 : 0.000\n",
      "        R@10 : 0.000\n",
      "       F1@10 : 0.000\n",
      "  success@10 : 0.000\n",
      "\n",
      "    stdout: [Warning] 114 queries in qrels but not in run. Examples: ['519', '1006', '477', '662', '120']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: dev2\n",
      "Queries (in qrels): 143\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf_reduced_queries\\eval20-queries-dev2.run\n",
      "     NDCG@10 : 0.003\n",
      "   NDCG@1000 : 0.019\n",
      "      R@1000 : 0.119\n",
      "          RR : 0.003\n",
      "         MAP : 0.003\n",
      "    MAP@1000 : 0.003\n",
      "        P@10 : 0.001\n",
      "        R@10 : 0.007\n",
      "       F1@10 : 0.001\n",
      "  success@10 : 0.007\n",
      "\n",
      "    stdout: [Warning] 428 queries in qrels but not in run. Examples: ['2001', '2002', '2003', '2005', '2006']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: dev3\n",
      "Queries (in qrels): 536\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf_reduced_queries\\eval20-queries-dev3.run\n",
      "     NDCG@10 : 0.081\n",
      "   NDCG@1000 : 0.093\n",
      "      R@1000 : 0.172\n",
      "          RR : 0.076\n",
      "         MAP : 0.076\n",
      "    MAP@1000 : 0.076\n",
      "        P@10 : 0.010\n",
      "        R@10 : 0.103\n",
      "       F1@10 : 0.019\n",
      "  success@10 : 0.103\n",
      "\n",
      "    stdout: [Warning] 114 queries in qrels but not in run. Examples: ['763', '802', '950', '220', '792']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: train\n",
      "Queries (in qrels): 143\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf_reduced_queries\\eval20-queries-train.run\n",
      "     NDCG@10 : 0.002\n",
      "   NDCG@1000 : 0.011\n",
      "      R@1000 : 0.070\n",
      "          RR : 0.001\n",
      "         MAP : 0.001\n",
      "    MAP@1000 : 0.001\n",
      "        P@10 : 0.001\n",
      "        R@10 : 0.007\n",
      "       F1@10 : 0.001\n",
      "  success@10 : 0.007\n",
      "\n",
      "Directory: ..\\data\\runs\\tf_idf_reduced_queries_weights\n",
      "    stdout: [Warning] 113 queries in qrels but not in run. Examples: ['531', '473', '659', '1095', '1038']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: dev1\n",
      "Queries (in qrels): 142\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf_reduced_queries_weights\\eval20-queries-dev1.run\n",
      "     NDCG@10 : 0.000\n",
      "   NDCG@1000 : 0.009\n",
      "      R@1000 : 0.070\n",
      "          RR : 0.000\n",
      "         MAP : 0.000\n",
      "    MAP@1000 : 0.000\n",
      "        P@10 : 0.000\n",
      "        R@10 : 0.000\n",
      "       F1@10 : 0.000\n",
      "  success@10 : 0.000\n",
      "\n",
      "    stdout: [Warning] 114 queries in qrels but not in run. Examples: ['519', '1006', '477', '662', '120']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: dev2\n",
      "Queries (in qrels): 143\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf_reduced_queries_weights\\eval20-queries-dev2.run\n",
      "     NDCG@10 : 0.012\n",
      "   NDCG@1000 : 0.027\n",
      "      R@1000 : 0.119\n",
      "          RR : 0.011\n",
      "         MAP : 0.011\n",
      "    MAP@1000 : 0.011\n",
      "        P@10 : 0.002\n",
      "        R@10 : 0.021\n",
      "       F1@10 : 0.004\n",
      "  success@10 : 0.021\n",
      "\n",
      "    stdout: [Warning] 428 queries in qrels but not in run. Examples: ['2001', '2002', '2003', '2005', '2006']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: dev3\n",
      "Queries (in qrels): 536\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf_reduced_queries_weights\\eval20-queries-dev3.run\n",
      "     NDCG@10 : 0.094\n",
      "   NDCG@1000 : 0.102\n",
      "      R@1000 : 0.170\n",
      "          RR : 0.086\n",
      "         MAP : 0.086\n",
      "    MAP@1000 : 0.086\n",
      "        P@10 : 0.012\n",
      "        R@10 : 0.123\n",
      "       F1@10 : 0.022\n",
      "  success@10 : 0.123\n",
      "\n",
      "    stdout: [Warning] 114 queries in qrels but not in run. Examples: ['763', '802', '950', '220', '792']\n",
      "Explanation: missing queries are scored as 0 and still averaged; this is standard TREC practice\n",
      "Since the metric values that also exist on the website match ours 1:1, they handled it the same way, and our pipeline corresponds to their evaluation method.\n",
      "\n",
      "Split: train\n",
      "Queries (in qrels): 143\n",
      "Run: C:\\Users\\Kai\\OneDrive\\Vorlesungen\\Information Retrieval\\Github\\IR_Project\\data\\runs\\tf_idf_reduced_queries_weights\\eval20-queries-train.run\n",
      "     NDCG@10 : 0.000\n",
      "   NDCG@1000 : 0.007\n",
      "      R@1000 : 0.049\n",
      "          RR : 0.000\n",
      "         MAP : 0.000\n",
      "    MAP@1000 : 0.000\n",
      "        P@10 : 0.000\n",
      "        R@10 : 0.000\n",
      "       F1@10 : 0.000\n",
      "  success@10 : 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "\n",
    "root_path = Path(OUT_PATH)\n",
    "\n",
    "for root, dirs, files in os.walk(root_path):\n",
    "    print(\"Directory:\", root)\n",
    "    for d in dirs:\n",
    "        print(\"  Subdirectory:\", os.path.join(root, d))\n",
    "    for f in files:\n",
    "        result = subprocess.run(f\"python ../EvalPipelineSubSet/run_eval.py --split {f.split(\".\")[0].split(\"-\")[-1]} --run {os.path.join(root, f)} --metrics ndcg@10 ndcg@1000 R@1000 rr map map@1000 P@10 R@10 f1@10 success@10\", shell=True, capture_output=True, text=True)\n",
    "        #print(\"  File:\", f, \"\\t|\", result.stdout )\n",
    "        #print(\"  File:\", f)\n",
    "        #print(\"    returncode:\", result.returncode)\n",
    "        print(\"    stdout:\", result.stdout)\n",
    "        #print(\"    stderr:\", result.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
